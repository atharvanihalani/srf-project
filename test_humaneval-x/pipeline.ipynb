{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e28b2a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c49d21",
   "metadata": {},
   "source": [
    "### Imports etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import inspect_ai\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "import contextlib\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "146feabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample, hf_dataset\n",
    "from inspect_ai.util import ExecResult, sandbox\n",
    "from inspect_ai.scorer import CORRECT, INCORRECT, Score, Scorer, Target, accuracy, scorer, stderr\n",
    "from inspect_ai.solver import TaskState, generate\n",
    "from inspect_ai.model import get_model\n",
    "from inspect_ai.log import read_eval_log\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "fae427e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/31/25 05:18:38] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Note: Environment variable`HF_TOKEN` is set and is the current active    <a href=\"file:///root/miniconda3/envs/srf-env/lib/python3.11/site-packages/huggingface_hub/_login.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_login.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///root/miniconda3/envs/srf-env/lib/python3.11/site-packages/huggingface_hub/_login.py#415\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">415</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         token independently from the token you've just configured.               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/31/25 05:18:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Note: Environment variable`HF_TOKEN` is set and is the current active    \u001b]8;id=570227;file:///root/miniconda3/envs/srf-env/lib/python3.11/site-packages/huggingface_hub/_login.py\u001b\\\u001b[2m_login.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=807203;file:///root/miniconda3/envs/srf-env/lib/python3.11/site-packages/huggingface_hub/_login.py#415\u001b\\\u001b[2m415\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         token independently from the token you've just configured.               \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])\n",
    "client = OpenAI()\n",
    "async_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_HELPER = {\n",
    "    \"python\": [\n",
    "        \"import math\",\n",
    "        \"import re\",\n",
    "        \"import sys\",\n",
    "        \"import copy\",\n",
    "        \"import datetime\",\n",
    "        \"import itertools\",\n",
    "        \"import collections\",\n",
    "        \"import heapq\",\n",
    "        \"import statistics\",\n",
    "        \"import functools\",\n",
    "        \"import hashlib\",\n",
    "        \"import numpy\",\n",
    "        \"import numpy as np\",\n",
    "        \"import string\",\n",
    "        \"from typing import *\",\n",
    "        \"from collections import *\",\n",
    "    ],\n",
    "    \"go\"    : [\n",
    "        \"math\",\n",
    "        \"strings\",\n",
    "        \"fmt\",\n",
    "        \"strconv\",\n",
    "        \"time\",\n",
    "        \"bytes\",\n",
    "        \"regexp\",\n",
    "        \"sort\",\n",
    "        \"math/rand\",\n",
    "        \"crypto/md5\",\n",
    "        \"encoding/hex\",\n",
    "    ],\n",
    "    \"cpp\"   : [\n",
    "        \"#include<stdlib.h>\",\n",
    "        \"#include<algorithm>\",\n",
    "        \"#include<math.h>\",\n",
    "        \"#include<stdio.h>\",\n",
    "        \"#include<vector>\",\n",
    "        \"#include<string>\",\n",
    "        \"#include<climits>\",\n",
    "        \"#include<cstring>\",\n",
    "        \"#include<iostream>\",\n",
    "        \"#include <numeric>\",\n",
    "        \"#include <sstream>\",\n",
    "        \"#include <stack>\",\n",
    "        \"#include <cctype>\",\n",
    "        \"#include <set>\",\n",
    "        \"#include <unordered_set>\",\n",
    "        \"#include <iomanip>\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# instruction prepended to code problem\n",
    "HUMANEVAL_INSTRUCTION = \"\"\"\n",
    "Read the following function signature and docstring, and fully implement\n",
    "the function described. Include a function signature, that's exactly the \n",
    "same as the one provided in the prompt. Do NOT include any explanations. \n",
    "Also do NOT include the `main` function\\n\n",
    "\"\"\"\n",
    "\n",
    "# JAVA\n",
    "\"\"\"\n",
    "Read the following function signature and docstring, and fully implement\n",
    "the method described. Ensure your method signature is exactly the same as \n",
    "the one provided in the prompt. Ensure you include the `class Solution {}` \n",
    "classname. Your response should only contain the code, no explanations. \n",
    "Example outputs are provided below. \n",
    "```java\n",
    "class Solution {\n",
    "    public void testMethod1() {\n",
    "        // method code\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "```java\n",
    "class Solution {\n",
    "    public bool testMethod2(int a, int b) {\n",
    "        // method code\n",
    "    }\n",
    "\n",
    "    public int helperMethod() {\n",
    "        // helper method code.\n",
    "    }\n",
    "}\n",
    "```\\n\n",
    "\"\"\"\n",
    "\n",
    "# EVERYTHING ELSE\n",
    "\"\"\"\n",
    "Read the following function signature and docstring, and fully implement\n",
    "the function described. Include a function signature, that's exactly the \n",
    "same as the one provided in the prompt. Do NOT include any explanations. \n",
    "Also do NOT include the `main` function\\n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "CODE_EXTRACTION_INSTRUCTION = \"\"\"\n",
    "Here is a section of code. Remove any leading package names, import statements, and comments. \n",
    "Leave any indentation as it is (ie. don't remove leading whitespace for any line).\\n\n",
    "\"\"\"\n",
    "\n",
    "# JAVA\n",
    "\"\"\"\n",
    "Here is a section of code. Remove any import statements and block comments. \n",
    "Leave any indentation as it is (ie. don't remove leading whitespace for any line).\\n\n",
    "\"\"\"\n",
    "\n",
    "# GO\n",
    "\"\"\"\n",
    "Here is a section of code. Remove any leading package names and comments. \n",
    "Leave any indentation as it is (ie. don't remove leading whitespace for any line).\n",
    "Also leave any import statements, if present.\\n\n",
    "\"\"\"\n",
    "\n",
    "# EVERYTHING ELSE\n",
    "\"\"\"\n",
    "Here is a section of code. Remove any leading package names, import statements, and comments. \n",
    "Leave any indentation as it is (ie. don't remove leading whitespace for any line).\\n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "LANG_PREFIX = {\n",
    "    \"cpp\"          : \"// language: C++\",\n",
    "    \"java\"         : \"// language: Java\",\n",
    "    \"js\"           : \"// language: JavaScript\",\n",
    "    \"javascript\"   : \"// language: JavaScript\",\n",
    "    \"go\"           : \"// language: Go\",\n",
    "    \"python\"       : \"# language: Python\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "ab12a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = get_model(\n",
    "        'hf/meta-llama/Llama-3.1-8B-Instruct', \n",
    "        device='auto',\n",
    "        torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_coder = get_model(\n",
    "    'hf/Qwen/Qwen2.5-Coder-7B-Instruct',\n",
    "    device=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725ebb6",
   "metadata": {},
   "source": [
    "### Language Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "bc5e47b2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# JavaScript\n",
    "#     apt-get update\n",
    "#     apt-get install -y curl\n",
    "#     curl -fsSL https://deb.nodesource.com/setup_lts.x | bash -\n",
    "#     apt-get install -y nodejs\n",
    "#     verify installation: `node -v` // `npm -v` // `node -e \"console.log('Node.js is working')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "0284d1fd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Java\n",
    "#     apt-get update\n",
    "#     apt-get install -y openjdk-21-jdk\n",
    "# Verify with:  \n",
    "#     java -version\n",
    "#     javac -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "ea4e9f29",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Go\n",
    "#     cd ~/\n",
    "#     GO_VERSION=1.24.5\n",
    "#     wget https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz\n",
    "#     rm -rf /usr/local/go\n",
    "#     tar -C /usr/local -xzf go${GO_VERSION}.linux-amd64.tar.gz\n",
    "#     echo 'export PATH=$PATH:/usr/local/go/bin' >> ~/.bashrc\n",
    "#     export PATH=$PATH:/usr/local/go/bin\n",
    "#     go version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "cbfd5e21",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# C++\n",
    "#     apt-get update\n",
    "#     apt-get install -y g++\n",
    "#     apt-get install -y build-essential\n",
    "#     apt-get install libboost-all-dev -y\n",
    "#     apt-get install libssl-dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c2a4c",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "f0c435c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl_all(filename: str):\n",
    "    results = []\n",
    "    fp = gzip.open(open(filename, \"rb\"), \"rt\")\n",
    "    for line in fp:\n",
    "        if any(not x.isspace() for x in line):\n",
    "            results.append(json.loads(line))\n",
    "    fp.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "b8d9696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_content = stream_jsonl_all('data/python_data.gz')\n",
    "cpp_content = stream_jsonl_all('data/cpp_data.gz')\n",
    "go_content = stream_jsonl_all('data/go_data.gz')\n",
    "java_content = stream_jsonl_all('data/java_data.gz')\n",
    "js_content = stream_jsonl_all('data/js_data.gz')\n",
    "content = [python_content, cpp_content, go_content, java_content, js_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "b5952979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n'"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = stream_jsonl_all('data/python_generations.gz')\n",
    "generations[0]['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "8a531c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n",
      "dict_keys(['task_id', 'prompt', 'import', 'docstring', 'declaration', 'canonical_solution', 'test', 'test_setup', 'example_test'])\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n"
     ]
    }
   ],
   "source": [
    "for lang in content:\n",
    "    print(lang[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c3a24",
   "metadata": {},
   "source": [
    "## Eval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f034f0",
   "metadata": {},
   "source": [
    "### Extract Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "edd2df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_codeblock(completion: str) -> str:\n",
    "    pattern_1 = re.compile(r\"```(?:python|javascript|java|cpp|go)\\n(.*?)```\", re.DOTALL)\n",
    "    pattern_2 = re.compile(r\"```\\n(.*?)```\", re.DOTALL)\n",
    "    matches = pattern_1.findall(completion) + pattern_2.findall(completion)\n",
    "\n",
    "    if matches == []:\n",
    "        return completion\n",
    "    else:\n",
    "        return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "f3b8d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def remove_signature(completion, lang):\n",
    "    prompt = CODE_EXTRACTION_INSTRUCTION + completion\n",
    "\n",
    "    response = await async_client.responses.create(\n",
    "        model='gpt-4.1-mini',\n",
    "        input=prompt,\n",
    "    )\n",
    "\n",
    "    text_out = response.output[-1].content[0].text\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def find_code(completion, lang):\n",
    "    processed = await remove_signature(completion, lang)\n",
    "    processed = identify_codeblock(processed)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9d892",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2920892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPP\n",
    "\n",
    "def get_final_cpp(state, completion):\n",
    "    imports = ''\n",
    "    for s in IMPORT_HELPER['cpp']:\n",
    "        if s not in state.metadata['prompt']:\n",
    "            imports += s + '\\n'\n",
    "    \n",
    "    prompt = state.metadata['prompt']\n",
    "    declaration = state.metadata['declaration']\n",
    "    header = declaration.strip().split('\\n')[-1]\n",
    "    updated_prompt = ''.join(prompt.split(header)[:-1])\n",
    "\n",
    "    code = imports + \"\\n\" + updated_prompt + completion + \"\\n\" + state.metadata['test']\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6747847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO\n",
    "\n",
    "def get_final_go(state, completion):\n",
    "    import_string = state.metadata['import']\n",
    "    prompt = state.metadata['prompt'].replace(import_string, '')\n",
    "    prompt = ''.join(prompt.split('func ')[:-1])\n",
    "\n",
    "    test = state.metadata['test']\n",
    "    test_setup = state.metadata['test_setup']\n",
    "    other_pkgs = []\n",
    "\n",
    "    for pkg in IMPORT_HELPER['go']:\n",
    "        if pkg not in test_setup:\n",
    "            p = pkg.split('/')[-1]\n",
    "            if p + '.' in completion:    \n",
    "                other_pkgs.append(f\"\\\"{pkg}\\\"\")\n",
    "    if other_pkgs:\n",
    "        import_other_pkgs = \"import (\\n\" + \"    \".join([p + \"\\n\" for p in other_pkgs]) + \")\"\n",
    "        final_code = test_setup + \"\\n\" + import_other_pkgs + \"\\n\" + prompt + completion + \"\\n\" + test\n",
    "    else:\n",
    "        final_code = test_setup + \"\\n\" + prompt + completion + \"\\n\" + test\n",
    "\n",
    "    return final_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAVA\n",
    "\n",
    "def get_final_java(state, completion):\n",
    "    prompt = state.metadata['prompt']\n",
    "    prompt = ''.join(prompt.split('class Solution')[:-1])\n",
    "\n",
    "    final_code = prompt + completion + \"\\n\\n\" + state.metadata['test'] + \"\\n\"\n",
    "    return final_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JS\n",
    "\n",
    "def get_final_js(state, completion):\n",
    "    prompt = state.metadata['prompt']\n",
    "    prompt = ''.join(prompt.split('const ')[:-1])\n",
    "\n",
    "    final_code = prompt + completion + \"\\n\\n\" + state.metadata['test'] + \"\\n\"\n",
    "    return final_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372bc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON\n",
    "\n",
    "def get_final_python(state, completion):\n",
    "    imports = \"\\n\".join(IMPORT_HELPER[\"python\"]) + \"\\n\"\n",
    "    prompt = state.metadata['prompt']\n",
    "    prompt = ''.join(prompt.split('def ')[:-1])\n",
    "\n",
    "    final_code = imports + prompt + completion + \"\\n\" + state.metadata['test'] + \"\\n\"\n",
    "    return final_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dffe12",
   "metadata": {},
   "source": [
    "### Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5278778",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_final(state, lang, task_id):\n",
    "    model_completion = state.output.completion\n",
    "    processed_completion = await find_code(model_completion, lang)\n",
    "\n",
    "    final = globals()[f'get_final_{lang}']\n",
    "    final_code = final(state, processed_completion)\n",
    "\n",
    "    if 'errormsg' in final_code:\n",
    "        print(f'error in sample: {task_id}')\n",
    "    \n",
    "    return model_completion, processed_completion, final_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ea676",
   "metadata": {},
   "source": [
    "### Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "f5785fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def python_scorer(final_code, *args):\n",
    "    try:\n",
    "        result = await sandbox().exec(\n",
    "            cmd=[\"python\", \"-c\", final_code],\n",
    "            timeout=30,\n",
    "        )\n",
    "    except TimeoutError:\n",
    "        result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "3a8fdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def js_scorer(final_code, *args):\n",
    "    try:\n",
    "        result = await sandbox().exec(\n",
    "            cmd=[\"node\", \"-e\", final_code],\n",
    "            timeout=30,\n",
    "        )\n",
    "    except TimeoutError:\n",
    "        result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "5a340183",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def go_scorer(final_code, idx, tmp_dir):\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "    file = os.path.join(tmp_dir, 'main_test.go')\n",
    "\n",
    "    with contextlib.chdir(tmp_dir):\n",
    "        open(file, 'w').write(final_code)\n",
    "        if not os.path.exists('go.mod'):\n",
    "            try:\n",
    "                subprocess.run(\n",
    "                    ['/usr/local/go/bin/go', 'mod', 'init', f'example.com/tmpmod_{idx}'],\n",
    "                    check=True, \n",
    "                    capture_output=True,\n",
    "                )\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(\"Error running go mod init:\")\n",
    "                print(e.stderr)\n",
    "        subprocess.run(\n",
    "            ['/usr/local/go/bin/go', 'mod', 'tidy'], \n",
    "            check=True, \n",
    "            capture_output=True,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        result = await sandbox().exec(\n",
    "            cmd=[\"/usr/local/go/bin/go\", \"test\", file],\n",
    "            timeout=30,\n",
    "            cwd=tmp_dir\n",
    "        )\n",
    "    except TimeoutError:\n",
    "        result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "bcafb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def java_scorer(final_code, idx, tmp_dir):\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "    file = os.path.join(tmp_dir, 'Main.java')\n",
    "\n",
    "    with contextlib.chdir(tmp_dir):\n",
    "        open(file, 'w').write(final_code)\n",
    "    \n",
    "    try:\n",
    "        compile_proc = subprocess.run(\n",
    "            [\"javac\", \"Main.java\"],\n",
    "            cwd=tmp_dir,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30  \n",
    "        )\n",
    "        if compile_proc.returncode != 0:\n",
    "            print(f\"Compilation failed! Idx: {idx}\")\n",
    "            print(\"stderr:\", compile_proc.stderr)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"Compilation timed out!\")\n",
    "    except Exception as e:\n",
    "        print(\"Compilation error:\", e)\n",
    "\n",
    "    try:\n",
    "        result = await sandbox().exec(\n",
    "            cmd=[\"java\", \"-cp\", tmp_dir, \"Main\"],\n",
    "            timeout=30,\n",
    "        )\n",
    "    except TimeoutError:\n",
    "        result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "797f6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cpp_scorer(final_code, idx, tmp_dir):\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "    file = os.path.join(tmp_dir, 'test.cpp')\n",
    "    executable = os.path.join(tmp_dir, 'test.out')\n",
    "\n",
    "    open(file, 'w').write(final_code)\n",
    "    \n",
    "    try:\n",
    "        compile_proc = subprocess.run(\n",
    "            [\"g++\", \"-std=c++17\", file, \"-o\", executable, '-lssl', '-lcrypto'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30  # seconds, adjust as needed\n",
    "        )\n",
    "        if compile_proc.returncode != 0:\n",
    "            print(f\"Compilation failed! task number: {idx}\")\n",
    "            print(\"stderr:\", compile_proc.stderr)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"Compilation timed out!\")\n",
    "\n",
    "    if os.path.exists(executable):\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[executable],\n",
    "                timeout=30\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "        except Exception as e:\n",
    "            print(f'execution failed cuz of: {e}')\n",
    "    else:\n",
    "        result = ExecResult(False, 1, \"\", \"Compiler Error\")\n",
    "\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f3b45",
   "metadata": {},
   "source": [
    "### Inspect Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "a859b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_idx(task_id: str):\n",
    "    lang, idx = task_id.split('/')\n",
    "    lang = lang.lower()\n",
    "    if lang == 'javascript':\n",
    "        lang = 'js'\n",
    "    idx = int(idx)\n",
    "\n",
    "    return lang, idx\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def main_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        lang, idx = get_lang_idx(task_id)\n",
    "\n",
    "        model_completion, processed_completion, final_code = await get_final(state, lang, task_id)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/tmp/test_{idx}/'\n",
    "        my_scorer = globals()[f'{lang}_scorer']\n",
    "        result = await my_scorer(final_code, idx, tmp_dir)\n",
    "\n",
    "        success = result.success and (result.stderr == '')\n",
    "        \n",
    "        return Score(\n",
    "            value=CORRECT if success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [final_code]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "            metadata={\n",
    "                'completion': model_completion,\n",
    "                'processed': processed_completion,\n",
    "                'final_code': final_code,\n",
    "                'idx': idx,\n",
    "                'task_id': task_id,\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "2f2b0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'python'\n",
    "\n",
    "def humaneval_record_to_sample(record):\n",
    "    model_input = HUMANEVAL_INSTRUCTION + LANG_PREFIX[language] + '\\n' + record['prompt'] \n",
    "\n",
    "    idx = int(record['task_id'].split('/')[-1])\n",
    "\n",
    "    metadata = {\n",
    "        \"prompt\": record[\"prompt\"],\n",
    "        \"test\": record[\"test\"],\n",
    "        \"declaration\": record[\"declaration\"]\n",
    "    }\n",
    "    if language == 'go':\n",
    "        metadata['import'] = go_content[idx]['import']\n",
    "        metadata['test_setup'] = go_content[idx]['test_setup']\n",
    "    \n",
    "    return Sample(\n",
    "        id=record[\"task_id\"],\n",
    "        input=model_input,\n",
    "        target=record[\"canonical_solution\"],\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "humaneval_dataset = hf_dataset(\n",
    "    path = 'THUDM/humaneval-x',\n",
    "    name = language,\n",
    "    split = 'test',\n",
    "    sample_fields = humaneval_record_to_sample,\n",
    "    trust = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "3a7d1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = 30\n",
    "\n",
    "@task\n",
    "def humaneval():\n",
    "    return Task(\n",
    "        dataset = humaneval_dataset,\n",
    "        solver = generate(),\n",
    "        scorer = main_scorer(),\n",
    "        sandbox = 'local',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9121b",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "dfee2300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02ca1c1b1754e99acc60d51f0a504fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1\n",
    "model = 'openai/gpt-4o-mini'\n",
    "\n",
    "inspect_ai.eval(\n",
    "    humaneval(), \n",
    "    model = model, \n",
    "    epochs = epochs,\n",
    "    log_dir = '/root/srf-project/test_humaneval-x/baseline_performance/python/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "b563774a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e4913058a343c8914a9d576210c2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1\n",
    "model = llama_model\n",
    "\n",
    "inspect_ai.eval(\n",
    "    humaneval(), \n",
    "    model = model, \n",
    "    epochs = epochs,\n",
    "    log_dir = '/root/srf-project/test_humaneval-x/baseline_performance/python/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "72d91d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749d9f028d99493a929b902a45e41872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1\n",
    "model = qwen_coder\n",
    "\n",
    "inspect_ai.eval(\n",
    "    humaneval(), \n",
    "    model = model, \n",
    "    epochs = epochs,\n",
    "    log_dir = '/root/srf-project/test_humaneval-x/baseline_performance/python/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/root/srf-project/test_humaneval-x/pipeline_check/python/llama_8b.eval'\n",
    "log = read_eval_log(path)\n",
    "\n",
    "def eval_duration(log):\n",
    "    start_time = log.stats.started_at\n",
    "    start_time = start_time.split('T')[-1].split('+')[0]\n",
    "\n",
    "    end_time = log.stats.completed_at\n",
    "    end_time = end_time.split('T')[-1].split('+')[0]\n",
    "    \n",
    "    fmt = \"%H:%M:%S\"\n",
    "    t1 = datetime.strptime(start_time, fmt)\n",
    "    t2 = datetime.strptime(end_time, fmt)\n",
    "\n",
    "    duration = (t2-t1).seconds\n",
    "\n",
    "    return f'{duration} seconds'\n",
    "\n",
    "def eval_score(log):\n",
    "    score = log.results.scores[0].metrics['accuracy'].value\n",
    "    return f'{score * 100:.2f}%'\n",
    "    \n",
    "print(eval_duration(log))\n",
    "print(eval_score(log))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

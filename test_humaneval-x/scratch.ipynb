{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import inspect_ai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import torch as t\n",
    "import subprocess\n",
    "import contextlib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a53e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample, hf_dataset\n",
    "from inspect_ai.util import ExecResult, sandbox\n",
    "from inspect_ai.scorer import CORRECT, INCORRECT, Score, Scorer, Target, accuracy, scorer, stderr\n",
    "from inspect_ai.solver import TaskState, generate\n",
    "from inspect_ai.model import get_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34683c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_HELPER = {\n",
    "    \"python\": [\n",
    "        \"import math\",\n",
    "        \"import re\",\n",
    "        \"import sys\",\n",
    "        \"import copy\",\n",
    "        \"import datetime\",\n",
    "        \"import itertools\",\n",
    "        \"import collections\",\n",
    "        \"import heapq\",\n",
    "        \"import statistics\",\n",
    "        \"import functools\",\n",
    "        \"import hashlib\",\n",
    "        \"import numpy\",\n",
    "        \"import numpy as np\",\n",
    "        \"import string\",\n",
    "        \"from typing import *\",\n",
    "        \"from collections import *\",\n",
    "    ],\n",
    "    \"go\"    : [\n",
    "        \"math\",\n",
    "        \"strings\",\n",
    "        \"fmt\",\n",
    "        \"strconv\",\n",
    "        \"time\",\n",
    "        \"bytes\",\n",
    "        \"regexp\",\n",
    "        \"sort\",\n",
    "        \"math/rand\",\n",
    "        \"crypto/md5\",\n",
    "    ],\n",
    "    \"cpp\"   : [\n",
    "        \"#include<stdlib.h>\",\n",
    "        \"#include<algorithm>\",\n",
    "        \"#include<math.h>\",\n",
    "        \"#include<stdio.h>\",\n",
    "        \"#include<vector>\",\n",
    "        \"#include<string>\",\n",
    "        \"#include<climits>\",\n",
    "        \"#include<cstring>\",\n",
    "        \"#include<iostream>\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264e082",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d97a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl_all(filename: str):\n",
    "    results = []\n",
    "    fp = gzip.open(open(filename, \"rb\"), \"rt\")\n",
    "    for line in fp:\n",
    "        if any(not x.isspace() for x in line):\n",
    "            results.append(json.loads(line))\n",
    "    fp.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e98c1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_content = stream_jsonl_all('data/python_data.gz')\n",
    "cpp_content = stream_jsonl_all('data/cpp_data.gz')\n",
    "go_content = stream_jsonl_all('data/go_data.gz')\n",
    "java_content = stream_jsonl_all('data/java_data.gz')\n",
    "js_content = stream_jsonl_all('data/js_data.gz')\n",
    "content = [python_content, cpp_content, go_content, java_content, js_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8c1e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = stream_jsonl_all('data/python_generations.gz')\n",
    "generations[0]['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08897aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'import', 'docstring', 'declaration', 'canonical_solution', 'test', 'test_setup', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in content:\n",
    "    print(lang[0].keys())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a2490",
   "metadata": {},
   "source": [
    "### LLM output => code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44909f",
   "metadata": {},
   "source": [
    "#### Inspect output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "981a5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_log = inspect_ai.log.read_eval_log('../logs/HumanEval_4o-mini.eval')\n",
    "llama_log = inspect_ai.log.read_eval_log('../logs/HumanEval_Llama-31-8b.eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79b6ca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verify': Score(value='C', answer='    \"\"\"\\n    Separate groups of balanced parentheses from a string.\\n\\n    Args:\\n    paren_string: A string containing multiple groups of nested parentheses.\\n\\n    Returns:\\n    A list of strings where each string represents a separate group of balanced parentheses.\\n    \"\"\"\\n    result = []\\n    current_group = \\'\\'\\n    balance = 0\\n    for char in paren_string:\\n        if char == \\'(\\':\\n            balance += 1\\n            current_group += char\\n        elif char == \\')\\':\\n            balance -= 1\\n            if balance == 0:\\n                result.append(current_group + char)\\n                current_group = \\'\\'\\n            else:\\n                current_group += char\\n        if balance < 0:\\n            raise ValueError(\"Unbalanced parentheses in the input string\")\\n    if balance != 0:\\n        raise ValueError(\"Unbalanced parentheses in the input string\")\\n    return result\\n', explanation='', metadata=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_log.samples[1].scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dab4036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama 3.1 8B:\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\"\n",
      "    Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n",
      "\n",
      "    Args:\n",
      "        numbers (List[float]): A list of floating point numbers.\n",
      "        threshold (float): The minimum distance between two numbers to be considered close.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if any two numbers are closer than the given threshold, False otherwise.\n",
      "    \"\"\"\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) <= threshold:\n",
      "                return True\n",
      "    return False\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# print(f'gpt 4o-mini:\\n\\n{openai_log.samples[0].output.choices[0].message.content}')\n",
    "# print('\\n\\n')\n",
    "print(f'llama 3.1 8B:\\n\\n{llama_log.samples[0].output.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openai_log.samples[0].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddf894",
   "metadata": {},
   "source": [
    "#### HumanEval-X output\n",
    "*now i have prompt template, feed it into model & look at output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880f609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"\"\"\n",
    "Read the following function signature and docstring, and fully implement\n",
    "the function described. Your response should only contain the code for\n",
    "this function.\\n\n",
    "\"\"\"\n",
    "\n",
    "LANG_PREFIX = {\n",
    "    \"cpp\"          : \"// language: C++\",\n",
    "    \"java\"         : \"// language: Java\",\n",
    "    \"js\"           : \"// language: JavaScript\",\n",
    "    \"javascript\"   : \"// language: JavaScript\",\n",
    "    \"go\"           : \"// language: Go\",\n",
    "    \"python\"       : \"# language: Python\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c32ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lang = 'python'\n",
    "model_input = INSTRUCTION + LANG_PREFIX[my_lang] + '\\n' + python_content[0]['prompt'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95c3020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  input=model_input\n",
    ")\n",
    "\n",
    "response_text = response.output[0].content[0].text\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71545e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', device_map='auto', torch_dtype=t.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', padding_side=\"left\", torch_dtype=t.bfloat16)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2831f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer([model_input], return_tensors='pt').to('cuda')\n",
    "generated_ids = model.generate(**tokenized_input, max_new_tokens=800)\n",
    "out = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc333fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read the following function signature and docstring, and fully implement\n",
      "the function described. Your response should only contain the code for\n",
      "this function.\n",
      "\n",
      "# language: Python\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6b8bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read the following function signature and docstring, and fully implement\n",
      "the function described. Your response should only contain the code for\n",
      "this function.\n",
      "\n",
      "# language: Python\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "# The function should return True if there are any two numbers closer to each other\n",
      "# than the threshold, and False otherwise. The function should not consider\n",
      "# duplicates in the list. It should also not consider a number with itself.\n",
      "\n",
      "# The function should work for any list of numbers and any positive threshold.\n",
      "# The function should also work for lists with less than two elements.\n",
      "# The function should also work for lists with no elements.\n",
      "\n",
      "# The function should work for any list of numbers, regardless of whether they are\n",
      "# integers or floats. The function should work for lists with negative numbers.\n",
      "# The function should work for lists with decimal numbers.\n",
      "\n",
      "# The function should work for any positive threshold. The function should not work\n",
      "# for a negative threshold, because the concept of \"closer\" does not apply to\n",
      "# negative numbers in this context.\n",
      "\n",
      "# The function should work for lists with numbers of different magnitudes. The\n",
      "# function should work for lists with numbers of the same magnitude but different\n",
      "# sign.\n",
      "\n",
      "# The function should work for lists with one element. The function should work for\n",
      "# lists with two elements.\n",
      "\n",
      "# The function should work for lists with three or more elements.\n",
      "\n",
      "# The function should work for empty lists. The function should work for lists with\n",
      "# duplicate elements.\n",
      "\n",
      "# The function should work for lists with zero.\n",
      "\n",
      "# The function should work for lists with one.\n",
      "\n",
      "# The function should work for lists with NaN.\n",
      "\n",
      "# The function should work for lists with Infinity.\n",
      "\n",
      "# The function should work for lists with negative Infinity.\n",
      "\n",
      "# The function should work for lists with a mix of these special values.\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    if not numbers:\n",
      "        return False\n",
      "    if threshold <= 0:\n",
      "        raise ValueError(\"Threshold must be a positive number\")\n",
      "    numbers = sorted(set(numbers))\n",
      "    for i in range(len(numbers) - 1):\n",
      "        if numbers[i + 1] - numbers[i] < threshold:\n",
      "            return True\n",
      "    return False\n",
      "# The function has been implemented according to the specifications. It first checks\n",
      "# if the list is empty, and if so, returns False. It then checks if the threshold is\n",
      "# positive, and if not, raises a ValueError. It then removes duplicates from the\n",
      "# list by converting it to a set and sorts the list. It then iterates over the list\n",
      "# and checks if the difference between two consecutive numbers is less than the\n",
      "# threshold. If it finds such a pair, it returns True. If it doesn't find any such\n",
      "# pair after iterating over the entire list, it returns False.  # The function has\n",
      "# been implemented according to the specifications. It first checks if the list is\n",
      "# empty, and if so, returns False. It then checks if the threshold is positive, and\n",
      "# if not, raises a ValueError. It then removes duplicates from the list by\n",
      "# converting it to a set and sorts the list. It then iterates over the list and\n",
      "# checks if the difference between two consecutive numbers is less than the\n",
      "# threshold. If it finds such a pair, it returns True. If it doesn't find any such\n",
      "# pair after iterating over the entire list, it returns False. \n",
      "# The function has been implemented according to the specifications. It first checks\n",
      "# if the list is empty, and if so, returns False. It then checks if the threshold is\n",
      "# positive, and if not, raises a ValueError. It then removes duplicates from the\n",
      "# list by converting it to a set and sorts the list. It then iterates over the list\n",
      "# and checks if the difference between two consecutive numbers is less than the\n",
      "# threshold. If it finds such a pair, it returns True. If it doesn't find any such\n",
      "# pair after iterating over the entire list, it returns False\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate this from model now lol.\n",
    "# dont be pussy.\n",
    "# two things. one generate from openai model\n",
    "# two, generate from hf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8551c95",
   "metadata": {},
   "source": [
    "relevant?\n",
    "- task id \n",
    "- prompt\n",
    "- canonical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4428ece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'Python/0',\n",
       " 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n',\n",
       " 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n',\n",
       " 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(has_close_elements):\\n    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\ncheck(has_close_elements)\",\n",
       " 'text': '    Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True',\n",
       " 'declaration': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n',\n",
       " 'example_test': 'def check(has_close_elements):\\n    assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False\\n    assert has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) == True\\ncheck(has_close_elements)\\n'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e61e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(has_close_elements):\n",
      "    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
      "    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
      "    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
      "    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
      "    assert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
      "    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
      "    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
      "\n",
      "check(has_close_elements)\n"
     ]
    }
   ],
   "source": [
    "# testing, python stuff\n",
    "for sample in python_content:\n",
    "    task_id = sample['task_id']\n",
    "    language = task_id.split('/')[0].lower()\n",
    "\n",
    "    prompt = sample['prompt']\n",
    "    test = python_content[0]['test']\n",
    "    break\n",
    "    # code = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a55013",
   "metadata": {},
   "source": [
    "### Execute Code  \n",
    "*assuming I have code, ensure I can execute it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a35b8fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n    >>> hasCloseElements(Arrays.asList(1.0, 2.0, 3.0), 0.5)\\n    false\\n    >>> hasCloseElements(Arrays.asList(1.0, 2.8, 3.0, 4.0, 5.0, 2.0), 0.3)\\n    true'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_content[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4455d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_executable_code(records):\n",
    "    code_list = []\n",
    "    for record in records:\n",
    "        code = [\n",
    "            record['prompt'],\n",
    "            record['canonical_solution'],\n",
    "            '\\n',\n",
    "            record['test'],\n",
    "        ]\n",
    "        code_list.append(''.join(code))\n",
    "    \n",
    "    return code_list\n",
    "\n",
    "def get_code_idx(idx):\n",
    "    out = get_executable_code(java_content[idx:idx+1])\n",
    "    return out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def java_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        code = get_code_idx(idx)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/java_tmp/test_{idx}/'\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "        file = os.path.join(tmp_dir, 'Main.java')\n",
    "\n",
    "        with contextlib.chdir(tmp_dir):\n",
    "            open(file, 'w').write(code)\n",
    "        \n",
    "        try:\n",
    "            compile_proc = subprocess.run(\n",
    "                [\"javac\", \"Main.java\"],\n",
    "                cwd=tmp_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30  # seconds, adjust as needed\n",
    "            )\n",
    "            if compile_proc.returncode != 0:\n",
    "                print(\"Compilation failed!\")\n",
    "                print(\"stderr:\", compile_proc.stderr)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Compilation timed out!\")\n",
    "        except Exception as e:\n",
    "            print(\"Compilation error:\", e)\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"java\", \"-cp\", tmp_dir, \"Main\"],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humaneval_record_to_sample(record):\n",
    "    model_input = 'hello world'\n",
    "    \n",
    "    return Sample(\n",
    "        id=record[\"task_id\"],\n",
    "        input=model_input,\n",
    "        target=record[\"canonical_solution\"],\n",
    "        metadata={\n",
    "            \"prompt\": record[\"prompt\"],\n",
    "            \"test\": record[\"test\"],\n",
    "            \"entry_point\": record[\"entry_point\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "humaneval_dataset = hf_dataset(\n",
    "    path = 'openai_humaneval',\n",
    "    split = 'test',\n",
    "    sample_fields = humaneval_record_to_sample,\n",
    "    trust = True,\n",
    ")\n",
    "\n",
    "@task\n",
    "def humaneval():\n",
    "    return Task(\n",
    "        dataset = humaneval_dataset,\n",
    "        solver = generate(),\n",
    "        scorer = java_scorer(),\n",
    "        sandbox = 'local',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ai.eval(humaneval(), model = 'openai/gpt-4o-mini', epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e2c66",
   "metadata": {},
   "source": [
    "#### Working Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee097694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAVA SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def java_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        code = get_code_idx(idx)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/java_tmp/test_{idx}/'\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "        file = os.path.join(tmp_dir, 'Main.java')\n",
    "\n",
    "        with contextlib.chdir(tmp_dir):\n",
    "            open(file, 'w').write(code)\n",
    "        \n",
    "        try:\n",
    "            compile_proc = subprocess.run(\n",
    "                [\"javac\", \"Main.java\"],\n",
    "                cwd=tmp_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30  # seconds, adjust as needed\n",
    "            )\n",
    "            if compile_proc.returncode != 0:\n",
    "                print(\"Compilation failed!\")\n",
    "                print(\"stderr:\", compile_proc.stderr)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Compilation timed out!\")\n",
    "        except Exception as e:\n",
    "            print(\"Compilation error:\", e)\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"java\", \"-cp\", tmp_dir, \"Main\"],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b9b3d",
   "metadata": {},
   "source": [
    "JAVA NOTES  \n",
    "- run the following code to install Java:  \n",
    "    `apt-get update`  \n",
    "    `apt-get install -y openjdk-17-jdk`  \n",
    "  verify with:  \n",
    "    `java -version`  \n",
    "    `javac -version`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def go_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        code = get_go_stuff_new(idx)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/go_tmp/test_{idx}/'\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "        file = os.path.join(tmp_dir, 'main_test.go')\n",
    "\n",
    "        with contextlib.chdir(tmp_dir):\n",
    "            open(file, 'w').write(code)\n",
    "            if not os.path.exists('go.mod'):\n",
    "                subprocess.run(['/usr/local/go/bin/go', 'mod', 'init', f'example.com/tmpmod_{idx}'], check=True)\n",
    "            subprocess.run(['/usr/local/go/bin/go', 'mod', 'tidy'], check=True)\n",
    "        \n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"/usr/local/go/bin/go\", \"test\", file],\n",
    "                timeout=30,\n",
    "                cwd=tmp_dir\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        # shutil.rmtree(tmp_dir)\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO PARSER\n",
    "\n",
    "def get_go_stuff_new(idx):\n",
    "    import_string = go_content[idx]['import']\n",
    "    prompt = go_content[idx]['prompt'].replace(import_string, '')\n",
    "    code = go_content[idx]['canonical_solution']        # FYI, change this to *generated code* later!\n",
    "\n",
    "    test = go_content[idx]['test']\n",
    "    test_setup = go_content[idx]['test_setup']\n",
    "    other_pkgs = []\n",
    "\n",
    "    for pkg in IMPORT_HELPER['go']:\n",
    "        if pkg not in test_setup:\n",
    "            p = pkg.split('/')[-1]\n",
    "            if p + '.' in code:    \n",
    "                other_pkgs.append(f\"\\\"{pkg}\\\"\")\n",
    "    if other_pkgs:\n",
    "        import_other_pkgs = \"import (\\n\" + \"    \".join([p + \"\\n\" for p in other_pkgs]) + \")\"\n",
    "        test_string = test_setup + \"\\n\" + import_other_pkgs + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "    else:\n",
    "        test_string = test_setup + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "\n",
    "    return test_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41ad7a",
   "metadata": {},
   "source": [
    "GO NOTES  \n",
    "- run the following shell code to install go  \n",
    "    `cd ~/`  \n",
    "    `GO_VERSION=1.24.5`  \n",
    "    `wget https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz`  \n",
    "    `rm -rf /usr/local/go`  \n",
    "    `tar -C /usr/local -xzf go${GO_VERSION}.linux-amd64.tar.gz`  \n",
    "    `echo 'export PATH=$PATH:/usr/local/go/bin' >> ~/.bashrc`  \n",
    "    `export PATH=$PATH:/usr/local/go/bin`  \n",
    "    `go version`  \n",
    "- you need to create separate Go package for *every* test case. Otherwise the `go.mod` files will interfere (cuz multi threading)  \n",
    "- also make sure you're running eval from the same directory (ie. specify the `cwd` flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "331e88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JS SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def js_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        ans = get_code_idx(idx)\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"node\", \"-e\", ans],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [ans]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2d759",
   "metadata": {},
   "source": [
    "JS NOTES  \n",
    "- remember to install node.js!  \n",
    "    `apt-get update`  \n",
    "    `apt-get install -y curl`  \n",
    "    `curl -fsSL https://deb.nodesource.com/setup_lts.x | bash -`  \n",
    "    `apt-get install -y nodejs`  \n",
    "    verify installation: `node -v` // `npm -v` // `node -e \"console.log('Node.js is working')\"`  \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8e40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def python_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        ans = get_code_idx(idx)\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"python\", \"-c\", ans],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [ans]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARSER: PYTHON + JS + JAVA \n",
    "\n",
    "def get_executable_code(records):\n",
    "    code_list = []\n",
    "    for record in records:\n",
    "        code = [\n",
    "            record['prompt'],\n",
    "            record['canonical_solution'],\n",
    "            '\\n',\n",
    "            record['test'],\n",
    "        ]\n",
    "    \n",
    "    return code_list\n",
    "\n",
    "def get_code_idx(idx):\n",
    "    out = get_executable_code(go_content[idx:idx+1])\n",
    "    return out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e5c3a",
   "metadata": {},
   "source": [
    "### Ignore RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_humaneval_test(sample, problems):\n",
    "    task_id = sample[\"task_id\"]\n",
    "    language = task_id.split(\"/\")[0].lower()\n",
    "\n",
    "    prompt = sample[\"prompt\"]\n",
    "    test = problems[task_id][\"test\"]\n",
    "    code = sample[\"generation\"]\n",
    "\n",
    "    # Pre-process for different languages\n",
    "    if language == \"python\":\n",
    "        code_ = []\n",
    "        for line in code.split(\"\\n\"):\n",
    "            if (len(line.strip()) > 0 and line[0] != ' ' and line[0] != '\\t'):\n",
    "                break\n",
    "            code_.append(line)\n",
    "        code = \"\\n\".join(code_)\n",
    "        test_setup = \"\\n\".join(IMPORT_HELPER[\"python\"]) + \"\\n\"\n",
    "        test_string = test_setup + prompt + code + \"\\n\" + test + \"\\n\"\n",
    "    elif language == \"cpp\":\n",
    "        test_set_up = \"\"\n",
    "        for s in IMPORT_HELPER[\"cpp\"]:\n",
    "            if s not in prompt:\n",
    "                test_set_up += s + \"\\n\"\n",
    "        test_string = test_set_up + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "    elif language == \"java\":\n",
    "        test_string = prompt + code + \"\\n\" + test\n",
    "    elif language == \"js\" or language == \"javascript\":\n",
    "        test_string = prompt + code + \"\\n\" + test\n",
    "    elif language == \"go\":\n",
    "        import_string = problems[task_id][\"import\"]\n",
    "        prompt = prompt.replace(import_string, \"\")\n",
    "        test = problems[task_id][\"test\"]\n",
    "        test_setup = problems[task_id][\"test_setup\"]\n",
    "        other_pkgs = []\n",
    "        for pkg in IMPORT_HELPER[\"go\"]:\n",
    "            if pkg not in test_setup:\n",
    "                p = pkg.split(\"/\")[-1]\n",
    "                if p + \".\" in code:\n",
    "                    other_pkgs.append(f\"\\\"{pkg}\\\"\")\n",
    "        if other_pkgs:\n",
    "            import_other_pkgs = \"import (\\n\" + \"    \".join([p + \"\\n\" for p in other_pkgs]) + \")\"\n",
    "            test_string = test_setup + \"\\n\" + import_other_pkgs + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "        else:\n",
    "            test_string = test_setup + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "\n",
    "    return test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_jsonl = python_content\n",
    "for sample in tqdm(sample_jsonl):\n",
    "    task_id = sample['task_id']\n",
    "    lang = task_id.split('/')[0].lower()\n",
    "    if lang == 'javascript': \n",
    "        lang = 'js'\n",
    "    \n",
    "    sample[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_functional_correctness(\n",
    "        input_file: str = None,\n",
    "        tmp_dir: str = \"./\",\n",
    "        n_workers: int = 32,\n",
    "        timeout: float = 500.0,\n",
    "        problem_file: str = \"../data/humaneval_python.jsonl.gz\",\n",
    "        out_dir: str = None,\n",
    "        k: List[int] = [1, 10, 100],\n",
    "):\n",
    "    \n",
    "    problems = read_dataset(problem_file,\n",
    "                            dataset_type=\"humaneval\")\n",
    "    sample_jsonl = stream_jsonl_all(input_file)\n",
    "\n",
    "    suffix = \"_results.jsonl\"\n",
    "    if out_dir is not None:\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_file = os.path.join(out_dir, input_file.split('/')[-1].replace(\".jsonl\", suffix))\n",
    "    else:\n",
    "        out_file = os.path.join(input_file.replace(\".jsonl\", suffix))\n",
    "\n",
    "    if \"/codegeex/benchmark/humaneval-x/\" in input_file:\n",
    "        test_groundtruth = True\n",
    "\n",
    "    if \"-to-\" in input_file:\n",
    "        translation_mode = True\n",
    "    else:\n",
    "        translation_mode = False\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "\n",
    "        futures = []\n",
    "        completion_id = Counter()\n",
    "        n_samples = 0\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        print(\"Reading samples...\")\n",
    "        for sample in tqdm(sample_jsonl):\n",
    "            task_id = sample[\"task_id\"]\n",
    "            lang = task_id.split(\"/\")[0].lower()\n",
    "            if translation_mode:\n",
    "                task_id = sample[\"task_id\"].split(\"/\")[-1]\n",
    "                lang = regex.findall(\"-to-.*-\", input_file)[0].split(\"-to-\")[-1].rstrip(\"-\")\n",
    "                for l in LANGUAGE_NAME:\n",
    "                    if l in lang:\n",
    "                        lang = l\n",
    "                        break\n",
    "                task_id = f\"{LANGUAGE_NAME[lang]}/{task_id}\"\n",
    "            if lang == \"javascript\":\n",
    "                lang = \"js\"\n",
    "            tmp_dir_ = os.path.join(tmp_dir, lang, \"evaluation\")\n",
    "            sample[\"task_id\"] = task_id\n",
    "            sample[\"test_code\"] = process_humaneval_test(sample, problems, False)\n",
    "            if sample[\"test_code\"] is None:\n",
    "                continue\n",
    "            if \"completion_id\" in sample:\n",
    "                completion_id_ = sample[\"completion_id\"]\n",
    "            else:\n",
    "                completion_id_ = completion_id[task_id]\n",
    "            args = (task_id, sample, lang, timeout, tmp_dir_, completion_id_)\n",
    "            future = executor.submit(check_correctness, *args)\n",
    "            futures.append(future)\n",
    "            completion_id[task_id] += 1\n",
    "            n_samples += 1\n",
    "\n",
    "        print(completion_id)\n",
    "        if len(completion_id) == len(problems):\n",
    "            evaluate_pass_at_k = True\n",
    "        else:\n",
    "            evaluate_pass_at_k = False\n",
    "\n",
    "        print(\"Running test suites...\")\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            result = future.result()\n",
    "            results[result[\"task_id\"]].append((result[\"completion_id\"], result))\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate pass@k.\n",
    "    total, correct = [], []\n",
    "    for result in results.values():\n",
    "        passed = [r[1][\"passed\"] for r in result]\n",
    "        total.append(len(passed))\n",
    "        correct.append(sum(passed))\n",
    "    total = np.array(total)\n",
    "    correct = np.array(correct)\n",
    "    if evaluate_pass_at_k:\n",
    "        ks = k\n",
    "        pass_at_k = {f\"pass@{k}\": estimate_pass_at_k(total, correct, k).mean()\n",
    "                     for k in ks if (total >= k).all()}\n",
    "        print(pass_at_k)\n",
    "    else:\n",
    "        print(\"Total:\", np.sum(total))\n",
    "        print(\"Correct:\", np.sum(correct))\n",
    "\n",
    "    print(\"Writing to: \", out_file)\n",
    "    if out_file.endswith(\".gz\"):\n",
    "        fp = gzip.GzipFile(fileobj=open(out_file, \"wb\"), mode=\"wb\")\n",
    "        for res in results.values():\n",
    "            for r in res:\n",
    "                fp.write((json.dumps(r[1]) + \"\\n\").encode(\"utf-8\"))\n",
    "    else:\n",
    "        fp = open(out_file, 'w')\n",
    "        for res in results.values():\n",
    "            for r in res:\n",
    "                fp.write(json.dumps(r[1]) + \"\\n\")\n",
    "    fp.close()\n",
    "\n",
    "    print(\"Evaluation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b38c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL – figure how humaneval-x generates / prompts model for code. compare w/ inspect.\n",
    "\n",
    "# with inspect, they\n",
    "#   a) prepend a default system prompt\n",
    "#   b) add the dataset['prompt']\n",
    "# \n",
    "# will this work for java, etc too? \n",
    "#   YA i think so. should be fine.\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl(filename: str):\n",
    "    \"\"\"\n",
    "    Parses each jsonl line and yields it as a dictionary\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as gzfp:\n",
    "        with gzip.open(gzfp, \"rt\") as fp:\n",
    "            for line in fp:\n",
    "                if any(not x.isspace() for x in line):\n",
    "                    yield json.loads(line)\n",
    "\n",
    "def read_dataset(data_file: str = None):\n",
    "    dataset = {task[\"task_id\"]: task for task in stream_jsonl(data_file)}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c69630",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/humaneval_python.jsonl.gz'\n",
    "out = read_dataset(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from pipeline.main import run_eval\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matharva_nihalani\u001b[0m (\u001b[33matharva_nihalani-brown-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a735291",
   "metadata": {},
   "source": [
    "## FT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c133",
   "metadata": {},
   "source": [
    "### Load Model / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be92a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=t.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token'''\n",
    "\n",
    "# model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct'\n",
    "model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=10000, test_size=500):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "filtered = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bde88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(record):\n",
    "    code = record['code']\n",
    "    tokens = tokenizer(\n",
    "        code, \n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "train_set = train_set.map(tokenize, batched=True, num_proc=32)\n",
    "train_set = train_set.select_columns(['input_ids', 'attention_mask'])\n",
    "\n",
    "test_set = test_set.map(tokenize, batched=True, num_proc=32)\n",
    "test_set = test_set.select_columns(['input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     idx = random.randint(0, dataset_cpp.num_rows)\n",
    "#     code = dataset_cpp[idx]['code']\n",
    "#     print(code)\n",
    "#     print('--x---x---x--\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = wandb.Api()\n",
    "# run = api.run(\"atharva_nihalani-brown-university/huggingface/diw9fexc\")\n",
    "# metrics_dataframe = run.history()\n",
    "# # metrics_dataframe.to_csv(\"metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the GPU metrics\n",
    "# gpu_memory_util = metrics_dataframe.get(\"gpu.0.memory\")  # For the first GPU\n",
    "# gpu_memory_alloc = metrics_dataframe.get(\"gpu.0.memoryAllocated\")\n",
    "# print(gpu_memory_util)\n",
    "# print(gpu_memory_alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38a454",
   "metadata": {},
   "source": [
    "### FT Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {'r': 4,  'alpha': 16, 'dropout': 0.00, 'accum_steps': 1, 'lr': 1e-4},   # A\n",
    "    {'r': 4,  'alpha': 32, 'dropout': 0.10, 'accum_steps': 2, 'lr': 2e-4},   # B\n",
    "    {'r': 8,  'alpha': 32, 'dropout': 0.05, 'accum_steps': 4, 'lr': 1e-4},   # C\n",
    "    {'r': 16, 'alpha': 64, 'dropout': 0.00, 'accum_steps': 1, 'lr': 5e-4},   # D\n",
    "    {'r': 16, 'alpha': 32, 'dropout': 0.10, 'accum_steps': 2, 'lr': 2e-4},   # E\n",
    "    {'r': 8,  'alpha': 64, 'dropout': 0.05, 'accum_steps': 1, 'lr': 5e-5},   # F\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41a11c",
   "metadata": {},
   "source": [
    "### FT #A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=configs[0]['r'],\n",
    "    lora_alpha=configs[0]['alpha'],\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=configs[0]['dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e45159",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=0.1,\n",
    "    eval_on_start=True,\n",
    "    # per_device_train_batch_size=8, \n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=configs[0]['accum_steps'],\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=configs[0]['lr'],\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='quantized',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae750b",
   "metadata": {},
   "source": [
    "### FT #B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=configs[1]['r'],\n",
    "    lora_alpha=configs[1]['alpha'],\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=configs[1]['dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=0.1,\n",
    "    eval_on_start=True,\n",
    "    # per_device_train_batch_size=8, \n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=configs[1]['accum_steps'],\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=configs[1]['lr'],\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='quantized',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1c276",
   "metadata": {},
   "source": [
    "### FT #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=configs[2]['r'],\n",
    "    lora_alpha=configs[2]['alpha'],\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=configs[2]['dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4757a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=0.1,\n",
    "    eval_on_start=True,\n",
    "    # per_device_train_batch_size=8, \n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=configs[2]['accum_steps'],\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=configs[2]['lr'],\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='quantized',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90465e",
   "metadata": {},
   "source": [
    "### RunEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bd49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/test_dir',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', args, samples=164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_new(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    shuffled = dataset.shuffle(seed=47).select(range(10500))\n",
    "\n",
    "    return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d577b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset_new('Java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc91e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dataset[300]['code']\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a047a",
   "metadata": {},
   "source": [
    "### Trial Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64568b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031387f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=0.1,\n",
    "    eval_on_start=True,\n",
    "    per_device_train_batch_size=8, \n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='temp_run',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(trial):\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    # model=model,\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_hp_space(trial):\n",
    "    return {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\"name\": \"objective\", \"goal\": \"minimize\"},\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 1e-6, \"max\": 1e-4},\n",
    "            \"per_device_train_batch_size\": {\"values\": [4, 8]},\n",
    "            \"gradient_accumulation_steps\": {\"values\": [1, 2, 4, 8]},\n",
    "            # \"r\": {\"values\": [2, 4, 8, 16]},\n",
    "            # \"lora_alpha\": {\"values\": [16, 32, 64, 128]},\n",
    "            # \"lora_dropout\": {\"min\": 0.0, \"max\": 0.2}, \n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials = trainer.hyperparameter_search( \n",
    "    direction=\"minimize\",\n",
    "    backend=\"wandb\",\n",
    "    hp_space=wandb_hp_space,\n",
    "    n_trials=4,\n",
    "    # compute_objective=compute_objective,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2d234",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3341af",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/llama3-java-finetune/checkpoint-1250',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', model_args=args, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ee715",
   "metadata": {},
   "source": [
    "### GPU Deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import gc\n",
    "\n",
    "free_memory, total_memory = t.cuda.mem_get_info()\n",
    "\n",
    "# Convert bytes to GB\n",
    "free_memory_gb = free_memory / (1024 * 1024 * 1024)\n",
    "total_memory_gb = total_memory / (1024 * 1024 * 1024)\n",
    "mem_used = t.cuda.device_memory_used() / (1024 ** 3)\n",
    "\n",
    "print(f\"Free GPU Memory: {free_memory_gb:.2f} GB\")\n",
    "print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "print(f'Memory Used: {mem_used:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e20428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

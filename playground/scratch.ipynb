{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from pipeline.main import run_eval\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matharva_nihalani\u001b[0m (\u001b[33matharva_nihalani-brown-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a735291",
   "metadata": {},
   "source": [
    "## FT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c133",
   "metadata": {},
   "source": [
    "### Load Model / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be92a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=t.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token'''\n",
    "\n",
    "# model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct'\n",
    "model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63aeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=10000, test_size=500):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "filtered = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ba17f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e948cdd6483748d0ae11d645700b2f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2e2643aee84f1a82025b6538cd69d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "train_set = train_set.map(add_description)\n",
    "test_set = test_set.map(add_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18d91be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d7fa4e808749229342bd09e6cebb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_final_prompt(row):\n",
    "    description = row['description'].strip()\n",
    "    code = row['code']\n",
    "\n",
    "    final_prompt = '\\n'.join([description, '<answer>', code, '</answer>'])\n",
    "    row['final_prompt'] = final_prompt\n",
    "\n",
    "    return row\n",
    "\n",
    "train_set = train_set.map(add_final_prompt)\n",
    "test_set = test_set.map(add_final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c10cd81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['p_id', 'language', 'status', 'code', 'description', 'final_prompt'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07bde88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd97adce3a0481cac1485c4072577fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (417257 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (406031 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (410544 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (458779 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (425275 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (357212 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (377097 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (453033 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (420812 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (431544 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (451747 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (437823 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (488186 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (422527 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (442751 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (448688 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (435767 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (444162 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (448931 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (450695 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (422370 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (438132 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (469774 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (455430 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (444017 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (491153 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (494811 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (460572 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (480405 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522431 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513831 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542899 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 6 named input_ids expected length 313 but got length 377097",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/root/miniconda3/envs/srf-env/lib/python3.11/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/root/miniconda3/envs/srf-env/lib/python3.11/site-packages/datasets/utils/py_utils.py\", line 688, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/root/miniconda3/envs/srf-env/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3540, in _map_single\n    writer.write_batch(batch, try_original_type=try_original_type)\n  File \"/root/miniconda3/envs/srf-env/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 629, in write_batch\n    pa_table = pa.Table.from_arrays(arrays, schema=schema)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/table.pxi\", line 4912, in pyarrow.lib.Table.from_arrays\n  File \"pyarrow/table.pxi\", line 4256, in pyarrow.lib.Table.validate\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Column 6 named input_ids expected length 313 but got length 377097\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m     tokens = tokenizer.apply_chat_template(msg, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m train_set = train_set.map(tokenize, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, num_proc=\u001b[32m32\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# train_set = train_set.select_columns(['input_ids', 'attention_mask'])\u001b[39;00m\n\u001b[32m     14\u001b[39m test_set = test_set.map(tokenize, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, num_proc=\u001b[32m32\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/datasets/arrow_dataset.py:3171\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3165\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3167\u001b[39m     unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3168\u001b[39m     total=pbar_total,\n\u001b[32m   3169\u001b[39m     desc=(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3170\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3171\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[32m   3172\u001b[39m         pool, Dataset._map_single, kwargs_iterable=kwargs_per_job\n\u001b[32m   3173\u001b[39m     ):\n\u001b[32m   3174\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m   3175\u001b[39m             shards_done += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/datasets/utils/py_utils.py:728\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    727\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m         [async_result.get(timeout=\u001b[32m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/datasets/utils/py_utils.py:728\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    727\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m         [async_result.get(timeout=\u001b[32m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/multiprocess/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mArrowInvalid\u001b[39m: Column 6 named input_ids expected length 313 but got length 377097"
     ]
    }
   ],
   "source": [
    "def tokenize(record):\n",
    "    final_prompt = record['final_prompt']\n",
    "    msg = [\n",
    "        {'role': 'user', 'content': final_prompt}\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer.apply_chat_template(msg, add_generation_prompt=True, return_dict=True)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "train_set = train_set.map(tokenize, batched=True, num_proc=32)\n",
    "# train_set = train_set.select_columns(['input_ids', 'attention_mask'])\n",
    "\n",
    "test_set = test_set.map(tokenize, batched=True, num_proc=32)\n",
    "# test_set = test_set.select_columns(['input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41a11c",
   "metadata": {},
   "source": [
    "### Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    # {'role': 'system', 'content': 'You are a friendly, helpful chatbot.'},\n",
    "    {'role': 'user', 'content': 'Hey, how are you?'},\n",
    "]\n",
    "\n",
    "out = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47594328",
   "metadata": {},
   "source": [
    "### FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.00,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e45159",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=0.1,\n",
    "    eval_on_start=True,\n",
    "    per_device_train_batch_size=8, \n",
    "    # auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='quantized',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90465e",
   "metadata": {},
   "source": [
    "### RunEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bd49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/test_dir',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', args, samples=164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_new(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    shuffled = dataset.shuffle(seed=47).select(range(10500))\n",
    "\n",
    "    return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d577b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset_new('Java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc91e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dataset[300]['code']\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2d234",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3341af",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/llama3-java-finetune/checkpoint-1250',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', model_args=args, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ee715",
   "metadata": {},
   "source": [
    "### GPU Deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import gc\n",
    "\n",
    "free_memory, total_memory = t.cuda.mem_get_info()\n",
    "\n",
    "# Convert bytes to GB\n",
    "free_memory_gb = free_memory / (1024 * 1024 * 1024)\n",
    "total_memory_gb = total_memory / (1024 * 1024 * 1024)\n",
    "mem_used = t.cuda.device_memory_used() / (1024 ** 3)\n",
    "\n",
    "print(f\"Free GPU Memory: {free_memory_gb:.2f} GB\")\n",
    "print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "print(f'Memory Used: {mem_used:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e20428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

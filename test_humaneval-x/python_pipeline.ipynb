{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import inspect_ai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import torch as t\n",
    "import subprocess\n",
    "import contextlib\n",
    "import shutil\n",
    "import ast\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a53e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample, hf_dataset\n",
    "from inspect_ai.util import ExecResult, sandbox\n",
    "from inspect_ai.scorer import CORRECT, INCORRECT, Score, Scorer, Target, accuracy, scorer, stderr\n",
    "from inspect_ai.solver import TaskState, generate\n",
    "from inspect_ai.model import get_model\n",
    "from inspect_ai.log import read_eval_log\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34683c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_HELPER = {\n",
    "    \"python\": [\n",
    "        \"import math\",\n",
    "        \"import re\",\n",
    "        \"import sys\",\n",
    "        \"import copy\",\n",
    "        \"import datetime\",\n",
    "        \"import itertools\",\n",
    "        \"import collections\",\n",
    "        \"import heapq\",\n",
    "        \"import statistics\",\n",
    "        \"import functools\",\n",
    "        \"import hashlib\",\n",
    "        \"import numpy\",\n",
    "        \"import numpy as np\",\n",
    "        \"import string\",\n",
    "        \"from typing import *\",\n",
    "        \"from collections import *\",\n",
    "    ],\n",
    "    \"go\"    : [\n",
    "        \"math\",\n",
    "        \"strings\",\n",
    "        \"fmt\",\n",
    "        \"strconv\",\n",
    "        \"time\",\n",
    "        \"bytes\",\n",
    "        \"regexp\",\n",
    "        \"sort\",\n",
    "        \"math/rand\",\n",
    "        \"crypto/md5\",\n",
    "    ],\n",
    "    \"cpp\"   : [\n",
    "        \"#include<stdlib.h>\",\n",
    "        \"#include<algorithm>\",\n",
    "        \"#include<math.h>\",\n",
    "        \"#include<stdio.h>\",\n",
    "        \"#include<vector>\",\n",
    "        \"#include<string>\",\n",
    "        \"#include<climits>\",\n",
    "        \"#include<cstring>\",\n",
    "        \"#include<iostream>\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# instruction prepended to code problem\n",
    "INSTRUCTION = \"\"\"\n",
    "Read the following function signature and docstring, and fully implement\n",
    "the function described. Your response should only contain the code for\n",
    "this function.\\n\n",
    "\"\"\"\n",
    "\n",
    "LANG_PREFIX = {\n",
    "    \"cpp\"          : \"// language: C++\",\n",
    "    \"java\"         : \"// language: Java\",\n",
    "    \"js\"           : \"// language: JavaScript\",\n",
    "    \"javascript\"   : \"// language: JavaScript\",\n",
    "    \"go\"           : \"// language: Go\",\n",
    "    \"python\"       : \"# language: Python\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46096290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9156e1be174007b678a52702b3fc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_model(\n",
    "        'hf/meta-llama/Llama-3.1-8B-Instruct', \n",
    "        device = 'auto',\n",
    "        torch_dtype=t.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264e082",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d97a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl_all(filename: str):\n",
    "    results = []\n",
    "    fp = gzip.open(open(filename, \"rb\"), \"rt\")\n",
    "    for line in fp:\n",
    "        if any(not x.isspace() for x in line):\n",
    "            results.append(json.loads(line))\n",
    "    fp.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98c1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_content = stream_jsonl_all('data/python_data.gz')\n",
    "cpp_content = stream_jsonl_all('data/cpp_data.gz')\n",
    "go_content = stream_jsonl_all('data/go_data.gz')\n",
    "java_content = stream_jsonl_all('data/java_data.gz')\n",
    "js_content = stream_jsonl_all('data/js_data.gz')\n",
    "content = [python_content, cpp_content, go_content, java_content, js_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8c1e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = stream_jsonl_all('data/python_generations.gz')\n",
    "generations[0]['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08897aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'import', 'docstring', 'declaration', 'canonical_solution', 'test', 'test_setup', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in content:\n",
    "    print(lang[0].keys())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a2490",
   "metadata": {},
   "source": [
    "### LLM output => code  \n",
    "*I need to see LLM output & transform that into code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce73ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND CODE\n",
    "\n",
    "def find_code_new(completion: str) -> str:\n",
    "    pattern_1 = re.compile(r\"```python\\n(.*?)```\", re.DOTALL)\n",
    "    pattern_2 = re.compile(r\"```\\n(.*?)```\", re.DOTALL)\n",
    "    matches = pattern_1.findall(completion) + pattern_2.findall(completion)\n",
    "    if matches:\n",
    "        extracted_answer = matches[0]\n",
    "        extracted_answer = extract_function_body_new(extracted_answer)\n",
    "    else:\n",
    "        extracted_answer = completion\n",
    "    return str(extracted_answer)\n",
    "\n",
    "\n",
    "def extract_function_body_new(code: str) -> str:\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in tree.body:\n",
    "            if not isinstance(node, ast.FunctionDef):\n",
    "                continue\n",
    "            code_lines = code.splitlines()\n",
    "            start = node.body[0].lineno - 1\n",
    "            end = node.body[-1].end_lineno\n",
    "            body_lines = code_lines[start:end]\n",
    "            return \"\\n\".join(body_lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting function body: {e}\")\n",
    "        return \"errormsg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f96eacc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = 8\n",
    "int(round(a ** (1. / 3))) ** 3 == a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922904de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def python_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        idx = state.sample_id\n",
    "        model_completion = state.output.completion\n",
    "        processed_completion = find_code_new(model_completion)\n",
    "        final_code = ''.join([\n",
    "            state.metadata[\"prompt\"],\n",
    "            processed_completion,\n",
    "            \"\\n\",\n",
    "            state.metadata[\"test\"],\n",
    "        ])\n",
    "\n",
    "        if 'errormsg' in processed_completion:\n",
    "            print(f'error in sample: {idx}')\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"python\", \"-c\", final_code],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [final_code]\n",
    "                + [f\"\\n\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "            metadata={\n",
    "                'completion': model_completion,\n",
    "                'processed': processed_completion,\n",
    "                'final_code': final_code,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d552c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset THUDM/humaneval-x from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c586998680468bb48b467e85955036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/105k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d09b0b33384c889720bc517e6d35d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d99ee4e06f548b2a2af3a9470e79ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang = 'python'\n",
    "\n",
    "def humaneval_record_to_sample(record):\n",
    "    model_input = INSTRUCTION + LANG_PREFIX[lang] + '\\n' + record['prompt'] \n",
    "    \n",
    "    return Sample(\n",
    "        id=record[\"task_id\"],\n",
    "        input=model_input,\n",
    "        target=record[\"canonical_solution\"],\n",
    "        metadata={\n",
    "            \"prompt\": record[\"prompt\"],\n",
    "            \"test\": record[\"test\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "humaneval_dataset = hf_dataset(\n",
    "    path = 'THUDM/humaneval-x',\n",
    "    name = lang,\n",
    "    split = 'test',\n",
    "    sample_fields = humaneval_record_to_sample,\n",
    "    trust = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d44cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 164\n",
    "\n",
    "@task\n",
    "def humaneval():\n",
    "    return Task(\n",
    "        dataset = humaneval_dataset[:samples],\n",
    "        solver = generate(),\n",
    "        scorer = python_scorer(),\n",
    "        sandbox = 'local',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ebd63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8844aae3436749af9a30eca32e4b2532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">error in sample: Python/35\n",
       "</pre>\n"
      ],
      "text/plain": [
       "error in sample: Python/35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error extracting function body: unmatched ')' (&lt;unknown&gt;, line 17)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error extracting function body: unmatched ')' (<unknown>, line 17)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">error in sample: Python/89\n",
       "</pre>\n"
      ],
      "text/plain": [
       "error in sample: Python/89\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1\n",
    "inspect_ai.eval(humaneval(), model = 'openai/gpt-4o-mini', epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "python_gpt_log = read_eval_log('/root/srf-project/test_humaneval-x/logs/python_gpt4o-mini.eval')\n",
    "model_out = python_gpt_log.samples[idx].messages[-1].content\n",
    "\n",
    "print(python_gpt_log.samples[idx].id)\n",
    "print()\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a933743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "\n",
      "```python\n",
      "def encrypt(s):\n",
      "    \"\"\"Create a function encrypt that takes a string as an argument and\n",
      "    returns a string encrypted with the alphabet being rotated. \n",
      "    The alphabet should be rotated in a manner such that the letters \n",
      "    shift down by two multiplied to two places.\n",
      "    For example:\n",
      "    encrypt('hi') returns 'lm'\n",
      "    encrypt('asdfghjkl') returns 'ewhjklnop'\n",
      "    encrypt('gf') returns 'kj'\n",
      "    encrypt('et') returns 'ix'\n",
      "    \"\"\"\n",
      "    result = []\n",
      "    for char in s:\n",
      "        if char.isalpha():  # Process only alphabetic characters\n",
      "            # Shift the character by 4 places in the alphabet\n",
      "            new_char = chr(((ord(char) - ord('a') + 4) % 26) + ord('a')) if char.islower() else \\\n",
      "                           ((ord(char) - ord('A') + 4) % 26) + ord('A'))\n",
      "            result.append(new_char)\n",
      "    return ''.join(result)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "target_id = 89\n",
    "idx = 0\n",
    "model_out = ''\n",
    "\n",
    "for i, sample in enumerate(python_gpt_log.samples):\n",
    "    id = int(sample.id.split('/')[-1])\n",
    "    if id == target_id:\n",
    "        idx = i\n",
    "        model_out = sample.messages[-1].content\n",
    "        print(i)\n",
    "        print()\n",
    "        print(model_out)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df5b1934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting function body: unmatched ')' (<unknown>, line 17)\n",
      "error msg\n"
     ]
    }
   ],
   "source": [
    "print(find_code_new(model_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcdbb8",
   "metadata": {},
   "source": [
    "#### Code Extraction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9c2d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```java\n",
      "import java.util.*;\n",
      "\n",
      "public class Solution {\n",
      "    /**\n",
      "     * Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n",
      "     *\n",
      "     * @param numbers A list of numbers to check\n",
      "     * @param threshold The minimum distance between two numbers to be considered as close\n",
      "     * @return True if any two numbers are closer than the threshold, False otherwise\n",
      "     */\n",
      "    public boolean hasCloseElements(List<Double> numbers, double threshold) {\n",
      "        if (numbers.size() < 2) {\n",
      "            return false;\n",
      "        }\n",
      "\n",
      "        for (int i = 0; i < numbers.size() - 1; i++) {\n",
      "            for (int j = i + 1; j < numbers.size(); j++) {\n",
      "                if (Math.abs(numbers.get(i) - numbers.get(j)) < threshold) {\n",
      "                    return true;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "\n",
      "        return false;\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "js_eval = read_eval_log('logs/java_1.eval')\n",
    "model_out = js_eval.samples[idx].messages[-1].content\n",
    "print(model_out)\n",
    "\n",
    "# print(data['completion'])\n",
    "# print(data['processed'])\n",
    "# print(data['final_code'])\n",
    "\n",
    "# find_code_new(data['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fa5f5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const testHasCloseElements = () => {\n",
      "  console.assert(hasCloseElements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) === true)\n",
      "  console.assert(\n",
      "    hasCloseElements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) === false\n",
      "  )\n",
      "  console.assert(hasCloseElements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) === true)\n",
      "  console.assert(hasCloseElements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) === false)\n",
      "  console.assert(hasCloseElements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) === true)\n",
      "  console.assert(hasCloseElements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) === true)\n",
      "  console.assert(hasCloseElements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) === false)\n",
      "}\n",
      "\n",
      "testHasCloseElements()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(js_content[0]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f9c4b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read the following function signature and docstring, and fully implement\n",
      "the function described. Your response should only contain the code for\n",
      "this function.\n",
      "\n",
      "// language: JavaScript\n",
      "/* Check if in given list of numbers, are any two numbers closer to each other than\n",
      "  given threshold.\n",
      "  >>> hasCloseElements([1.0, 2.0, 3.0], 0.5)\n",
      "  false\n",
      "  >>> hasCloseElements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "  true\n",
      "  */\n",
      "const hasCloseElements = (numbers, threshold) => {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(js_eval.samples[0].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc36ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess â€“ get 5 evals files!\n",
    "num_eval_logs = 5\n",
    "eval_logs = []\n",
    "for i in range(num_eval_logs):\n",
    "    file_name = f'logs/test_{i}.eval'\n",
    "    eval_logs.append(read_eval_log(file_name))\n",
    "\n",
    "samples_read = 164\n",
    "for i in range(samples_read):\n",
    "    processed_results = []\n",
    "    for j in range(num_eval_logs):\n",
    "        data = eval_logs[0].samples[i].scores['verify'].metadata\n",
    "        processed_results.append(data['processed'])\n",
    "    \n",
    "    assert len(set(processed_results)) == 1\n",
    "\n",
    "# if this works, easy to scale it up to 164 samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import inspect_ai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import torch as t\n",
    "import subprocess\n",
    "import contextlib\n",
    "import shutil\n",
    "import ast\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a53e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample, hf_dataset\n",
    "from inspect_ai.util import ExecResult, sandbox\n",
    "from inspect_ai.scorer import CORRECT, INCORRECT, Score, Scorer, Target, accuracy, scorer, stderr\n",
    "from inspect_ai.solver import TaskState, generate\n",
    "from inspect_ai.model import get_model\n",
    "from inspect_ai.log import read_eval_log\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34683c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_HELPER = {\n",
    "    \"python\": [\n",
    "        \"import math\",\n",
    "        \"import re\",\n",
    "        \"import sys\",\n",
    "        \"import copy\",\n",
    "        \"import datetime\",\n",
    "        \"import itertools\",\n",
    "        \"import collections\",\n",
    "        \"import heapq\",\n",
    "        \"import statistics\",\n",
    "        \"import functools\",\n",
    "        \"import hashlib\",\n",
    "        \"import numpy\",\n",
    "        \"import numpy as np\",\n",
    "        \"import string\",\n",
    "        \"from typing import *\",\n",
    "        \"from collections import *\",\n",
    "    ],\n",
    "    \"go\"    : [\n",
    "        \"math\",\n",
    "        \"strings\",\n",
    "        \"fmt\",\n",
    "        \"strconv\",\n",
    "        \"time\",\n",
    "        \"bytes\",\n",
    "        \"regexp\",\n",
    "        \"sort\",\n",
    "        \"math/rand\",\n",
    "        \"crypto/md5\",\n",
    "    ],\n",
    "    \"cpp\"   : [\n",
    "        \"#include<stdlib.h>\",\n",
    "        \"#include<algorithm>\",\n",
    "        \"#include<math.h>\",\n",
    "        \"#include<stdio.h>\",\n",
    "        \"#include<vector>\",\n",
    "        \"#include<string>\",\n",
    "        \"#include<climits>\",\n",
    "        \"#include<cstring>\",\n",
    "        \"#include<iostream>\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# instruction prepended to code problem\n",
    "INSTRUCTION = \"\"\"\n",
    "Read the following function signature and docstring, and fully implement\n",
    "the function described. Your response should only contain the code for\n",
    "this function.\\n\n",
    "\"\"\"\n",
    "\n",
    "LANG_PREFIX = {\n",
    "    \"cpp\"          : \"// language: C++\",\n",
    "    \"java\"         : \"// language: Java\",\n",
    "    \"js\"           : \"// language: JavaScript\",\n",
    "    \"javascript\"   : \"// language: JavaScript\",\n",
    "    \"go\"           : \"// language: Go\",\n",
    "    \"python\"       : \"# language: Python\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46096290",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(\n",
    "        'hf/meta-llama/Llama-3.1-8B-Instruct', \n",
    "        device = 'auto',\n",
    "        torch_dtype=t.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264e082",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl_all(filename: str):\n",
    "    results = []\n",
    "    fp = gzip.open(open(filename, \"rb\"), \"rt\")\n",
    "    for line in fp:\n",
    "        if any(not x.isspace() for x in line):\n",
    "            results.append(json.loads(line))\n",
    "    fp.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_content = stream_jsonl_all('data/python_data.gz')\n",
    "cpp_content = stream_jsonl_all('data/cpp_data.gz')\n",
    "go_content = stream_jsonl_all('data/go_data.gz')\n",
    "java_content = stream_jsonl_all('data/java_data.gz')\n",
    "js_content = stream_jsonl_all('data/js_data.gz')\n",
    "content = [python_content, cpp_content, go_content, java_content, js_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = stream_jsonl_all('data/python_generations.gz')\n",
    "generations[0]['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in content:\n",
    "    print(lang[0].keys())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a2490",
   "metadata": {},
   "source": [
    "### LLM output => code  \n",
    "*I need to see LLM output & transform that into code*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d89607",
   "metadata": {},
   "source": [
    "#### Find Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7613c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_code_new(completion: str) -> str:\n",
    "    pattern_1 = re.compile(r\"```python\\n(.*?)```\", re.DOTALL)\n",
    "    pattern_2 = re.compile(r\"```\\n(.*?)```\", re.DOTALL)\n",
    "    matches = pattern_1.findall(completion) + pattern_2.findall(completion)\n",
    "    if matches:\n",
    "        extracted_answer = matches[0]\n",
    "        extracted_answer = extract_function_body_new(extracted_answer)\n",
    "    else:\n",
    "        extracted_answer = completion\n",
    "    return str(extracted_answer)\n",
    "\n",
    "\n",
    "def extract_function_body_new(code: str) -> str:\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in tree.body:\n",
    "            if not isinstance(node, ast.FunctionDef):\n",
    "                continue\n",
    "            code_lines = code.splitlines()\n",
    "            start = node.body[0].lineno - 1\n",
    "            end = node.body[-1].end_lineno\n",
    "            body_lines = code_lines[start:end]\n",
    "            return \"\\n\".join(body_lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting function body: {e}\")\n",
    "        return \"error msg\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f6027",
   "metadata": {},
   "source": [
    "#### Run Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d94d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def verify() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        idx = state.sample_id\n",
    "        model_completion = state.output.completion\n",
    "        processed_completion = find_code_new(model_completion)\n",
    "\n",
    "        val = CORRECT\n",
    "        if 'error' in processed_completion:\n",
    "            print(f'error in sample: {idx}')\n",
    "            val = INCORRECT\n",
    "\n",
    "        final_code = [\n",
    "            state.metadata[\"prompt\"],\n",
    "            processed_completion,\n",
    "            \"\\n\",\n",
    "            state.metadata[\"test\"],\n",
    "        ]\n",
    "\n",
    "        return Score(\n",
    "            value=val,\n",
    "            metadata={\n",
    "                'completion': model_completion,\n",
    "                'processed': processed_completion,\n",
    "                'final_code': ''.join(final_code),\n",
    "                'idx': idx\n",
    "            },\n",
    "            answer=f'model completion: {model_completion}',\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d552c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'python'\n",
    "\n",
    "def humaneval_record_to_sample(record):\n",
    "    model_input = INSTRUCTION + LANG_PREFIX[lang] + '\\n' + record['prompt'] \n",
    "    \n",
    "    return Sample(\n",
    "        id=record[\"task_id\"],\n",
    "        input=model_input,\n",
    "        target=record[\"canonical_solution\"],\n",
    "        metadata={\n",
    "            \"prompt\": record[\"prompt\"],\n",
    "            \"test\": record[\"test\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "humaneval_dataset = hf_dataset(\n",
    "    path = 'THUDM/humaneval-x',\n",
    "    name = lang,\n",
    "    split = 'test',\n",
    "    sample_fields = humaneval_record_to_sample,\n",
    "    trust = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def humaneval():\n",
    "    return Task(\n",
    "        dataset = humaneval_dataset,\n",
    "        solver = generate(),\n",
    "        scorer = verify(),\n",
    "        sandbox = 'local',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "inspect_ai.eval(humaneval(), model = model, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcdbb8",
   "metadata": {},
   "source": [
    "#### Code Extraction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_eval_log('logs/llama.eval')\n",
    "data = test.samples[52].scores['verify'].metadata\n",
    "completion = data['completion']\n",
    "processed = data['processed']\n",
    "final = data['final_code']\n",
    "idx = data['idx']\n",
    "\n",
    "idx\n",
    "# 34, 51, 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(164) if test.samples[i].scores['verify'].value != 'C' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5876a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc36ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess â€“ get 5 evals files!\n",
    "num_eval_logs = 5\n",
    "eval_logs = []\n",
    "for i in range(num_eval_logs):\n",
    "    file_name = f'logs/test_{i}.eval'\n",
    "    eval_logs.append(read_eval_log(file_name))\n",
    "\n",
    "samples_read = 164\n",
    "for i in range(samples_read):\n",
    "    processed_results = []\n",
    "    for j in range(num_eval_logs):\n",
    "        data = eval_logs[0].samples[i].scores['verify'].metadata\n",
    "        processed_results.append(data['processed'])\n",
    "    \n",
    "    assert len(set(processed_results)) == 1\n",
    "\n",
    "# if this works, easy to scale it up to 164 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c38ec6",
   "metadata": {},
   "source": [
    "#### Working Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa18d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON\n",
    "\n",
    "def find_code_new(completion: str) -> str:\n",
    "    pattern_1 = re.compile(r\"```python\\n(.*?)```\", re.DOTALL)\n",
    "    pattern_2 = re.compile(r\"```\\n(.*?)```\", re.DOTALL)\n",
    "    matches = pattern_1.findall(completion) + pattern_2.findall(completion)\n",
    "    if matches:\n",
    "        extracted_answer = matches[0]\n",
    "        extracted_answer = extract_function_body_new(extracted_answer)\n",
    "    else:\n",
    "        extracted_answer = completion\n",
    "    return str(extracted_answer)\n",
    "\n",
    "\n",
    "def extract_function_body_new(code: str) -> str:\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        for node in tree.body:\n",
    "            if not isinstance(node, ast.FunctionDef):\n",
    "                continue\n",
    "            code_lines = code.splitlines()\n",
    "            start = node.body[0].lineno - 1\n",
    "            end = node.body[-1].end_lineno\n",
    "            body_lines = code_lines[start:end]\n",
    "            return \"\\n\".join(body_lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting function body: {e}\")\n",
    "        return \"error msg\"\n",
    "    \n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def verify() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        idx = state.sample_id\n",
    "        model_completion = state.output.completion\n",
    "        processed_completion = find_code_new(model_completion)\n",
    "\n",
    "        val = CORRECT\n",
    "        if 'error' in processed_completion:\n",
    "            print(f'error in sample: {idx}')\n",
    "            val = INCORRECT\n",
    "\n",
    "        final_code = [\n",
    "            state.metadata[\"prompt\"],\n",
    "            processed_completion,\n",
    "            \"\\n\",\n",
    "            state.metadata[\"test\"],\n",
    "        ]\n",
    "\n",
    "        return Score(\n",
    "            value=val,\n",
    "            metadata={\n",
    "                'completion': model_completion,\n",
    "                'processed': processed_completion,\n",
    "                'final_code': ''.join(final_code),\n",
    "                'idx': idx\n",
    "            },\n",
    "            answer=f'model completion: {model_completion}',\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a55013",
   "metadata": {},
   "source": [
    "### Execute Code  \n",
    "*assuming I have code, ensure I can execute it*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094dd8e8",
   "metadata": {},
   "source": [
    "#### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_content[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39add97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpp(idx):\n",
    "    record = cpp_content[idx]\n",
    "    test_setup = ''\n",
    "    for s in IMPORT_HELPER['cpp']:\n",
    "        if s not in record['prompt']:\n",
    "            test_setup += s + '\\n'\n",
    "\n",
    "    code = test_setup + \"\\n\" + record['prompt'] + record['canonical_solution'] + \"\\n\" + record['test']\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def cpp_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        code = get_cpp(idx)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/cpp_tmp/test_{idx}/'\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "        file = os.path.join(tmp_dir, 'test.cpp')\n",
    "        executable = os.path.join(tmp_dir, 'test.out')\n",
    "\n",
    "        open(file, 'w').write(code)\n",
    "        \n",
    "        try:\n",
    "            compile_proc = subprocess.run(\n",
    "                [\"g++\", \"-std=c++17\", file, \"-o\", executable, '-lssl', '-lcrypto'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30  # seconds, adjust as needed\n",
    "            )\n",
    "            if compile_proc.returncode != 0:\n",
    "                print(f\"Compilation failed! task number: {idx}\")\n",
    "                print(\"stderr:\", compile_proc.stderr)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Compilation timed out!\")\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[executable],\n",
    "                timeout=30\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "        except Exception as e:\n",
    "            print(f'execution failed cuz of: {e}')\n",
    "\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humaneval_record_to_sample(record):\n",
    "    model_input = 'hello world'\n",
    "    \n",
    "    return Sample(\n",
    "        id=record[\"task_id\"],\n",
    "        input=model_input,\n",
    "        target=record[\"canonical_solution\"],\n",
    "        metadata={\n",
    "            \"prompt\": record[\"prompt\"],\n",
    "            \"test\": record[\"test\"],\n",
    "            \"entry_point\": record[\"entry_point\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "humaneval_dataset = hf_dataset(\n",
    "    path = 'openai_humaneval',\n",
    "    split = 'test',\n",
    "    sample_fields = humaneval_record_to_sample,\n",
    "    trust = True,\n",
    ")\n",
    "\n",
    "@task\n",
    "def humaneval():\n",
    "    return Task(\n",
    "        dataset = humaneval_dataset,\n",
    "        solver = generate(),\n",
    "        scorer = cpp_scorer(),\n",
    "        sandbox = 'local',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ai.eval(humaneval(), model = 'openai/gpt-4o-mini', epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e2c66",
   "metadata": {},
   "source": [
    "#### Working Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPP SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def cpp_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        code = get_cpp(idx)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/cpp_tmp/test_{idx}/'\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "        file = os.path.join(tmp_dir, 'test.cpp')\n",
    "        executable = os.path.join(tmp_dir, 'test.out')\n",
    "\n",
    "        open(file, 'w').write(code)\n",
    "        \n",
    "        try:\n",
    "            compile_proc = subprocess.run(\n",
    "                [\"g++\", \"-std=c++17\", file, \"-o\", executable, '-lssl', '-lcrypto'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30  # seconds, adjust as needed\n",
    "            )\n",
    "            if compile_proc.returncode != 0:\n",
    "                print(f\"Compilation failed! task number: {idx}\")\n",
    "                print(\"stderr:\", compile_proc.stderr)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Compilation timed out!\")\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[executable],\n",
    "                timeout=30\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "        except Exception as e:\n",
    "            print(f'execution failed cuz of: {e}')\n",
    "\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPP PARSER\n",
    "\n",
    "def get_cpp(idx):\n",
    "    record = cpp_content[idx]\n",
    "    test_setup = ''\n",
    "    for s in IMPORT_HELPER['cpp']:\n",
    "        if s not in record['prompt']:\n",
    "            test_setup += s + '\\n'\n",
    "\n",
    "    code = test_setup + \"\\n\" + record['prompt'] + record['canonical_solution'] + \"\\n\" + record['test']\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066a406",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# CPP NOTES\n",
    "\n",
    "# apt-get update\n",
    "# apt-get install -y g++\n",
    "# apt-get install -y build-essential\n",
    "# apt-get install libboost-all-dev\n",
    "# apt-get install libssl-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee097694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAVA SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def java_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        code = get_code_idx(idx)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/java_tmp/test_{idx}/'\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "        file = os.path.join(tmp_dir, 'Main.java')\n",
    "\n",
    "        with contextlib.chdir(tmp_dir):\n",
    "            open(file, 'w').write(code)\n",
    "        \n",
    "        try:\n",
    "            compile_proc = subprocess.run(\n",
    "                [\"javac\", \"Main.java\"],\n",
    "                cwd=tmp_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30  # seconds, adjust as needed\n",
    "            )\n",
    "            if compile_proc.returncode != 0:\n",
    "                print(\"Compilation failed!\")\n",
    "                print(\"stderr:\", compile_proc.stderr)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Compilation timed out!\")\n",
    "        except Exception as e:\n",
    "            print(\"Compilation error:\", e)\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"java\", \"-cp\", tmp_dir, \"Main\"],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        shutil.rmtree(tmp_dir)\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b9b3d",
   "metadata": {},
   "source": [
    "JAVA NOTES  \n",
    "- run the following code to install Java:  \n",
    "    `apt-get update`  \n",
    "    `apt-get install -y openjdk-17-jdk`  \n",
    "  verify with:  \n",
    "    `java -version`  \n",
    "    `javac -version`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def go_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        code = get_go_stuff_new(idx)\n",
    "\n",
    "        tmp_dir = f'/root/srf-project/test_humaneval-x/go_tmp/test_{idx}/'\n",
    "        if not os.path.exists(tmp_dir):\n",
    "            os.mkdir(tmp_dir)\n",
    "        file = os.path.join(tmp_dir, 'main_test.go')\n",
    "\n",
    "        with contextlib.chdir(tmp_dir):\n",
    "            open(file, 'w').write(code)\n",
    "            if not os.path.exists('go.mod'):\n",
    "                subprocess.run(['/usr/local/go/bin/go', 'mod', 'init', f'example.com/tmpmod_{idx}'], check=True)\n",
    "            subprocess.run(['/usr/local/go/bin/go', 'mod', 'tidy'], check=True)\n",
    "        \n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"/usr/local/go/bin/go\", \"test\", file],\n",
    "                timeout=30,\n",
    "                cwd=tmp_dir\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        # shutil.rmtree(tmp_dir)\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO PARSER\n",
    "\n",
    "def get_go_stuff_new(idx):\n",
    "    import_string = go_content[idx]['import']\n",
    "    prompt = go_content[idx]['prompt'].replace(import_string, '')\n",
    "    code = go_content[idx]['canonical_solution']        # FYI, change this to *generated code* later!\n",
    "\n",
    "    test = go_content[idx]['test']\n",
    "    test_setup = go_content[idx]['test_setup']\n",
    "    other_pkgs = []\n",
    "\n",
    "    for pkg in IMPORT_HELPER['go']:\n",
    "        if pkg not in test_setup:\n",
    "            p = pkg.split('/')[-1]\n",
    "            if p + '.' in code:    \n",
    "                other_pkgs.append(f\"\\\"{pkg}\\\"\")\n",
    "    if other_pkgs:\n",
    "        import_other_pkgs = \"import (\\n\" + \"    \".join([p + \"\\n\" for p in other_pkgs]) + \")\"\n",
    "        test_string = test_setup + \"\\n\" + import_other_pkgs + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "    else:\n",
    "        test_string = test_setup + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "\n",
    "    return test_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41ad7a",
   "metadata": {},
   "source": [
    "GO NOTES  \n",
    "- run the following shell code to install go  \n",
    "    `cd ~/`  \n",
    "    `GO_VERSION=1.24.5`  \n",
    "    `wget https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz`  \n",
    "    `rm -rf /usr/local/go`  \n",
    "    `tar -C /usr/local -xzf go${GO_VERSION}.linux-amd64.tar.gz`  \n",
    "    `echo 'export PATH=$PATH:/usr/local/go/bin' >> ~/.bashrc`  \n",
    "    `export PATH=$PATH:/usr/local/go/bin`  \n",
    "    `go version`  \n",
    "- you need to create separate Go package for *every* test case. Otherwise the `go.mod` files will interfere (cuz multi threading)  \n",
    "- also make sure you're running eval from the same directory (ie. specify the `cwd` flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JS SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def js_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        ans = get_code_idx(idx)\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"node\", \"-e\", ans],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [ans]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2d759",
   "metadata": {},
   "source": [
    "JS NOTES  \n",
    "- remember to install node.js!  \n",
    "    `apt-get update`  \n",
    "    `apt-get install -y curl`  \n",
    "    `curl -fsSL https://deb.nodesource.com/setup_lts.x | bash -`  \n",
    "    `apt-get install -y nodejs`  \n",
    "    verify installation: `node -v` // `npm -v` // `node -e \"console.log('Node.js is working')\"`  \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON SCORER\n",
    "\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def python_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        task_id = state.sample_id\n",
    "        idx = int(task_id.split('/')[1])\n",
    "        ans = get_code_idx(idx)\n",
    "\n",
    "        try:\n",
    "            result = await sandbox().exec(\n",
    "                cmd=[\"python\", \"-c\", ans],\n",
    "                timeout=30,\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            result = ExecResult(False, 1, \"\", \"Verification timed out.\")\n",
    "\n",
    "        return Score(\n",
    "            value=CORRECT if result.success else INCORRECT,\n",
    "            explanation=\"\".join(\n",
    "                [\"The following verification code was executed:\\n\\n\"]\n",
    "                + [\"```python\\n\\n\"]\n",
    "                + [ans]\n",
    "                + [\"\\n```\\n\"]\n",
    "                + [f\"\\nThe submission was incorrect\\n\\n{result.stderr}\"]\n",
    "                if not result.success\n",
    "                else [\"\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARSER: PYTHON + JS + JAVA \n",
    "\n",
    "def get_executable_code(records):\n",
    "    code_list = []\n",
    "    for record in records:\n",
    "        code = [\n",
    "            record['prompt'],\n",
    "            record['canonical_solution'],\n",
    "            '\\n',\n",
    "            record['test'],\n",
    "        ]\n",
    "    \n",
    "    return code_list\n",
    "\n",
    "def get_code_idx(idx):\n",
    "    out = get_executable_code(go_content[idx:idx+1])\n",
    "    return out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e5c3a",
   "metadata": {},
   "source": [
    "### Ignore RN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth # ABOVE PYTORCH (cuz dependency things)\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from pipeline.main import run_eval\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matharva_nihalani\u001b[0m (\u001b[33matharva_nihalani-brown-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a735291",
   "metadata": {},
   "source": [
    "## FT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a02b1",
   "metadata": {},
   "source": [
    "### Unsloth Experimentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fc4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45c6d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.4: Fast Llama patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA H100 NVL. Num GPUs = 1. Max memory: 93.111 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f65a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.8.4 patched 32 layers with 32 QKV layers, 32 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a431eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=10000, test_size=500):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "filtered = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8af6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "train_set = train_set.map(add_description)\n",
    "test_set = test_set.map(add_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1744ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c063c21a56c24ad89019092857e4ad65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1e467e0e1640b897f51377e0f6c72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_final_prompt_new(row):\n",
    "    preface = 'Read the following problem description. Fully implement a solution in Java. Your response should only contain the code, no explanations.\\n\\n'\n",
    "    problem_description = row['description']\n",
    "    code = row['code']\n",
    "\n",
    "    conversational_prompt_completion = {\n",
    "        'prompt': [{\n",
    "            'role': 'user',\n",
    "            'content': preface + problem_description\n",
    "        }],\n",
    "        'completion': [{\n",
    "            'role': 'assistant',\n",
    "            'content': code\n",
    "        }],\n",
    "    }\n",
    "\n",
    "    return conversational_prompt_completion\n",
    "\n",
    "final_train_set = train_set.map(add_final_prompt_new, remove_columns=['description', 'code', 'p_id', 'status', 'language'])\n",
    "final_test_set = test_set.map(add_final_prompt_new, remove_columns=['description', 'code', 'p_id', 'status', 'language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b744d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19090075",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        completion_only_loss=True,\n",
    "        dataset_kwargs={'skip_prepare_dataset': True}\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = final_train_set,\n",
    "    # max_seq_length = max_seq_length,\n",
    "    # packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = sft_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "061b27b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature: (input_ids, labels, seq_lengths, completion_mask, assistant_masks). The following columns have been ignored: [prompt, completion]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   2239\u001b[39m         args=args,\n\u001b[32m   2240\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m   2241\u001b[39m         trial=trial,\n\u001b[32m   2242\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   2243\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/accelerate/utils/memory.py:174\u001b[39m, in \u001b[36mfind_executable_batch_size.<locals>.decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo executable batch size found, reached zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m function(batch_size, *args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:23\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:1060\u001b[39m, in \u001b[36mTrainer.get_train_dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTrainer: training requires a train_dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_dataloader(\n\u001b[32m   1061\u001b[39m     dataset=\u001b[38;5;28mself\u001b[39m.train_dataset,\n\u001b[32m   1062\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1063\u001b[39m     batch_size=\u001b[38;5;28mself\u001b[39m._train_batch_size,\n\u001b[32m   1064\u001b[39m     sampler_fn=\u001b[38;5;28mself\u001b[39m._get_train_sampler,\n\u001b[32m   1065\u001b[39m     is_training=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1066\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:1015\u001b[39m, in \u001b[36mTrainer._get_dataloader\u001b[39m\u001b[34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[39m\n\u001b[32m   1013\u001b[39m data_collator = \u001b[38;5;28mself\u001b[39m.data_collator\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets.Dataset):\n\u001b[32m-> \u001b[39m\u001b[32m1015\u001b[39m     dataset = \u001b[38;5;28mself\u001b[39m._remove_unused_columns(dataset, description=description)\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1017\u001b[39m     data_collator = \u001b[38;5;28mself\u001b[39m._get_collator_with_removed_columns(\u001b[38;5;28mself\u001b[39m.data_collator, description=description)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:941\u001b[39m, in \u001b[36mTrainer._remove_unused_columns\u001b[39m\u001b[34m(self, dataset, description)\u001b[39m\n\u001b[32m    939\u001b[39m columns = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset.column_names]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    942\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo columns in the dataset match the model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms forward method signature: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    943\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    944\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    945\u001b[39m     )\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(datasets.__version__) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m1.4.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    948\u001b[39m     dataset.set_format(\n\u001b[32m    949\u001b[39m         \u001b[38;5;28mtype\u001b[39m=dataset.format[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m], columns=columns, format_kwargs=dataset.format[\u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    950\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No columns in the dataset match the model's forward method signature: (input_ids, labels, seq_lengths, completion_mask, assistant_masks). The following columns have been ignored: [prompt, completion]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c133",
   "metadata": {},
   "source": [
    "### Load Model / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be92a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=t.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token'''\n",
    "\n",
    "# model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct'\n",
    "model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=10000, test_size=500):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "filtered = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(filtered, test_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "train_set = train_set.map(add_description)\n",
    "test_set = test_set.map(add_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d91be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_final_prompt(row):\n",
    "    description = row['description'].strip()\n",
    "    code = row['code']\n",
    "\n",
    "    final_prompt = '\\n'.join([description, '<answer>', code, '</answer>'])\n",
    "    row['final_prompt'] = final_prompt\n",
    "\n",
    "    return row\n",
    "\n",
    "train_set = train_set.map(add_final_prompt)\n",
    "test_set = test_set.map(add_final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bde88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(record):\n",
    "    final_prompt = record['final_prompt']\n",
    "    msg = [\n",
    "        {'role': 'user', 'content': final_prompt}\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer.apply_chat_template(msg, add_generation_prompt=True, return_dict=True)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "final_train_set = train_set.map(tokenize, num_proc=32)\n",
    "final_test_set = test_set.map(tokenize, num_proc=32)\n",
    "\n",
    "final_train_set = final_train_set.select_columns(['input_ids', 'attention_mask'])\n",
    "final_test_set = final_test_set.select_columns(['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47594328",
   "metadata": {},
   "source": [
    "### FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.00,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e45159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir():\n",
    "    idx = 0\n",
    "    base_path = '/root/srf-project/playground/llama3-java-finetune-'\n",
    "    already_existing = True\n",
    "\n",
    "    while already_existing:\n",
    "        idx += 1\n",
    "        path = base_path + str(idx)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            already_existing = False\n",
    "            os.mkdir(path)\n",
    "            return path\n",
    "        \n",
    "output_path = get_save_dir()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=0.1,\n",
    "    # eval_on_start=True,\n",
    "    per_device_train_batch_size=1, \n",
    "    # auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # gradient_checkpointing=True,\n",
    "    torch_empty_cache_steps=1,\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='chat_template',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_train_set,\n",
    "    eval_dataset=final_test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90465e",
   "metadata": {},
   "source": [
    "### RunEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bd49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/playground/llama3-java-finetune-1',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', args, samples=164)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2d234",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3341af",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/llama3-java-finetune/checkpoint-1250',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', model_args=args, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ee715",
   "metadata": {},
   "source": [
    "### GPU Deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import gc\n",
    "\n",
    "free_memory, total_memory = t.cuda.mem_get_info()\n",
    "\n",
    "# Convert bytes to GB\n",
    "free_memory_gb = free_memory / (1024 * 1024 * 1024)\n",
    "total_memory_gb = total_memory / (1024 * 1024 * 1024)\n",
    "mem_used = t.cuda.device_memory_used() / (1024 ** 3)\n",
    "\n",
    "print(f\"Free GPU Memory: {free_memory_gb:.2f} GB\")\n",
    "print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "print(f'Memory Used: {mem_used:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e20428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

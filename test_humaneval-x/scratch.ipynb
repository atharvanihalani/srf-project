{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from pipeline.main import run_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a735291",
   "metadata": {},
   "source": [
    "## FT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c133",
   "metadata": {},
   "source": [
    "### Load Model / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be92a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=t.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token'''\n",
    "\n",
    "model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct'\n",
    "# model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(lang, train_size=10000, test_size=500):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "train_set, test_set = get_datasets('Java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bde88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(record):\n",
    "    code = record['code']\n",
    "    tokens = tokenizer(\n",
    "        code, \n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "train_set = train_set.map(tokenize, batched=True, num_proc=32)\n",
    "train_set = train_set.select_columns(['input_ids', 'attention_mask'])\n",
    "\n",
    "test_set = test_set.map(tokenize, batched=True, num_proc=32)\n",
    "test_set = test_set.select_columns(['input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     idx = random.randint(0, dataset_cpp.num_rows)\n",
    "#     code = dataset_cpp[idx]['code']\n",
    "#     print(code)\n",
    "#     print('--x---x---x--\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = wandb.Api()\n",
    "# run = api.run(\"atharva_nihalani-brown-university/huggingface/diw9fexc\")\n",
    "# metrics_dataframe = run.history()\n",
    "# # metrics_dataframe.to_csv(\"metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the GPU metrics\n",
    "# gpu_memory_util = metrics_dataframe.get(\"gpu.0.memory\")  # For the first GPU\n",
    "# gpu_memory_alloc = metrics_dataframe.get(\"gpu.0.memoryAllocated\")\n",
    "# print(gpu_memory_util)\n",
    "# print(gpu_memory_alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41a11c",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64568b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031387f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=0.1,\n",
    "    eval_on_start=True,\n",
    "    per_device_train_batch_size=8, \n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='temp_run',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(trial):\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    # model=model,\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_hp_space(trial):\n",
    "    return {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\"name\": \"objective\", \"goal\": \"minimize\"},\n",
    "        \"parameters\": {\n",
    "            \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 1e-6, \"max\": 1e-4},\n",
    "            \"per_device_train_batch_size\": {\"values\": [4, 8]},\n",
    "            \"gradient_accumulation_steps\": {\"values\": [1, 2, 4, 8]},\n",
    "            # \"r\": {\"values\": [2, 4, 8, 16]},\n",
    "            # \"lora_alpha\": {\"values\": [16, 32, 64, 128]},\n",
    "            # \"lora_dropout\": {\"min\": 0.0, \"max\": 0.2}, \n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials = trainer.hyperparameter_search( \n",
    "    direction=\"minimize\",\n",
    "    backend=\"wandb\",\n",
    "    hp_space=wandb_hp_space,\n",
    "    n_trials=4,\n",
    "    # compute_objective=compute_objective,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2d234",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3341af",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/test_humaneval-x/llama3-java-finetune/checkpoint-1250',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', model_args=args, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ee715",
   "metadata": {},
   "source": [
    "### GPU Deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import gc\n",
    "\n",
    "free_memory, total_memory = t.cuda.mem_get_info()\n",
    "\n",
    "# Convert bytes to GB\n",
    "free_memory_gb = free_memory / (1024 * 1024 * 1024)\n",
    "total_memory_gb = total_memory / (1024 * 1024 * 1024)\n",
    "mem_used = t.cuda.device_memory_used() / (1024 ** 3)\n",
    "\n",
    "print(f\"Free GPU Memory: {free_memory_gb:.2f} GB\")\n",
    "print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "print(f'Memory Used: {mem_used:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e20428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt, get_chat_template, train_on_responses_only\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, AutoTokenizer, AutoModelForCausalLM, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig \n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import wandb\n",
    "import json\n",
    "from trl import SFTTrainer #, SFTConfig\n",
    "from pipeline.main import run_eval\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# from pipeline.main import run_eval\n",
    "# from IPython.display import display, HTML\n",
    "# import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matharva_nihalani\u001b[0m (\u001b[33matharva_nihalani-brown-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb794de9",
   "metadata": {},
   "source": [
    "## Test RunEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb4a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/root/srf-project/playground/llama3-java-finetune-a/checkpoint-625', device_map='auto', torch_dtype='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912c9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('test_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da2cd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d43354387b40f091f3c8a9f36b2e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 27\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:7: error: no suitable method found for reduce(String,(a,b)-&gt;a + b)\n",
       "        return string.chars().mapToObj(c -&gt; Character.isLowerCase(c) ? Character.toUpperCase(c) : \n",
       "Character.toLowerCase(c)).reduce(\"\", (a, b) -&gt; a + b);\n",
       "                                                                                                                   \n",
       "        ^\n",
       "    method Stream.reduce(Integer,BinaryOperator&lt;Integer&gt;) is not applicable\n",
       "      (argument mismatch; String cannot be converted to Integer)\n",
       "    method Stream.&lt;U&gt;reduce(U,BiFunction&lt;U,? super Integer,U&gt;,BinaryOperator&lt;U&gt;) is not applicable\n",
       "      (cannot infer type-variable(s) U\n",
       "        (actual and formal argument lists differ in length))\n",
       "  where U,T are type-variables:\n",
       "    U extends Object declared in method &lt;U&gt;reduce(U,BiFunction&lt;U,? super T,U&gt;,BinaryOperator&lt;U&gt;)\n",
       "    T extends Object declared in interface Stream\n",
       "Note: Some messages have been simplified; recompile with -Xdiags:verbose to get full output\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:7: error: no suitable method found for reduce(String,(a,b)->a + b)\n",
       "        return string.chars().mapToObj(c -> Character.isLowerCase(c) ? Character.toUpperCase(c) : \n",
       "Character.toLowerCase(c)).reduce(\"\", (a, b) -> a + b);\n",
       "                                                                                                                   \n",
       "        ^\n",
       "    method Stream.reduce(Integer,BinaryOperator<Integer>) is not applicable\n",
       "      (argument mismatch; String cannot be converted to Integer)\n",
       "    method Stream.<U>reduce(U,BiFunction<U,? super Integer,U>,BinaryOperator<U>) is not applicable\n",
       "      (cannot infer type-variable(s) U\n",
       "        (actual and formal argument lists differ in length))\n",
       "  where U,T are type-variables:\n",
       "    U extends Object declared in method <U>reduce(U,BiFunction<U,? super T,U>,BinaryOperator<U>)\n",
       "    T extends Object declared in interface Stream\n",
       "Note: Some messages have been simplified; recompile with -Xdiags:verbose to get full output\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 50\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "Main.java:32: error: cannot find symbol\n",
       "            String encode_str = s.encodeShift(str);\n",
       "                                 ^\n",
       "  symbol:   method encodeShift(String)\n",
       "  location: variable s of type Solution\n",
       "2 errors\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "Main.java:32: error: cannot find symbol\n",
       "            String encode_str = s.encodeShift(str);\n",
       "                                 ^\n",
       "  symbol:   method encodeShift(String)\n",
       "  location: variable s of type Solution\n",
       "2 errors\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 38\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:36: error: cannot find symbol\n",
       "            String encode_str = s.encodeCyclic(str);\n",
       "                                 ^\n",
       "  symbol:   method encodeCyclic(String)\n",
       "  location: variable s of type Solution\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:36: error: cannot find symbol\n",
       "            String encode_str = s.encodeCyclic(str);\n",
       "                                 ^\n",
       "  symbol:   method encodeCyclic(String)\n",
       "  location: variable s of type Solution\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 76\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 76\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 86\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 86\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 90\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 90\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "Main.java:9: error: cannot find symbol\n",
       "        List&lt;Integer&gt; sorted = lst.stream().sorted().collect(Collectors.toList());\n",
       "                                                             ^\n",
       "  symbol:   variable Collectors\n",
       "  location: class Solution\n",
       "2 errors\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "Main.java:9: error: cannot find symbol\n",
       "        List<Integer> sorted = lst.stream().sorted().collect(Collectors.toList());\n",
       "                                                             ^\n",
       "  symbol:   variable Collectors\n",
       "  location: class Solution\n",
       "2 errors\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 99\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 99\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 107\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 107\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:10: error: cannot find symbol\n",
       "            if (s.equals(s.reverse())) {\n",
       "                          ^\n",
       "  symbol:   method reverse()\n",
       "  location: variable s of type String\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:10: error: cannot find symbol\n",
       "            if (s.equals(s.reverse())) {\n",
       "                          ^\n",
       "  symbol:   method reverse()\n",
       "  location: variable s of type String\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 124\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 124\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 144\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 144\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "Main.java:6: error: cannot find symbol\n",
       "        BigDecimal b1 = new BigDecimal(x);\n",
       "        ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:6: error: cannot find symbol\n",
       "        BigDecimal b1 = new BigDecimal(x);\n",
       "                            ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:7: error: cannot find symbol\n",
       "        BigDecimal b2 = new BigDecimal(n);\n",
       "        ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:7: error: cannot find symbol\n",
       "        BigDecimal b2 = new BigDecimal(n);\n",
       "                            ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:8: error: cannot find symbol\n",
       "        return b1.multiply(b2).remainder(new BigDecimal(\"1\")).equals(new BigDecimal(\"0\"));\n",
       "                                                                         ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:8: error: cannot find symbol\n",
       "        return b1.multiply(b2).remainder(new BigDecimal(\"1\")).equals(new BigDecimal(\"0\"));\n",
       "                                             ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "7 errors\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "Main.java:6: error: cannot find symbol\n",
       "        BigDecimal b1 = new BigDecimal(x);\n",
       "        ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:6: error: cannot find symbol\n",
       "        BigDecimal b1 = new BigDecimal(x);\n",
       "                            ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:7: error: cannot find symbol\n",
       "        BigDecimal b2 = new BigDecimal(n);\n",
       "        ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:7: error: cannot find symbol\n",
       "        BigDecimal b2 = new BigDecimal(n);\n",
       "                            ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:8: error: cannot find symbol\n",
       "        return b1.multiply(b2).remainder(new BigDecimal(\"1\")).equals(new BigDecimal(\"0\"));\n",
       "                                                                         ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "Main.java:8: error: cannot find symbol\n",
       "        return b1.multiply(b2).remainder(new BigDecimal(\"1\")).equals(new BigDecimal(\"0\"));\n",
       "                                             ^\n",
       "  symbol:   class BigDecimal\n",
       "  location: class Solution\n",
       "7 errors\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 139\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 139\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:8: error: cannot find symbol\n",
       "            result *= BigInteger.valueOf(i).longValue();\n",
       "                      ^\n",
       "  symbol:   variable BigInteger\n",
       "  location: class Solution\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:8: error: cannot find symbol\n",
       "            result *= BigInteger.valueOf(i).longValue();\n",
       "                      ^\n",
       "  symbol:   variable BigInteger\n",
       "  location: class Solution\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 129\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 129\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:43: error: incompatible types: inference variable T has incompatible bounds\n",
       "        return Arrays.asList(path);\n",
       "                            ^\n",
       "    equality constraints: Integer\n",
       "    lower bounds: int[]\n",
       "  where T is a type-variable:\n",
       "    T extends Object declared in method &lt;T&gt;asList(T...)\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:43: error: incompatible types: inference variable T has incompatible bounds\n",
       "        return Arrays.asList(path);\n",
       "                            ^\n",
       "    equality constraints: Integer\n",
       "    lower bounds: int[]\n",
       "  where T is a type-variable:\n",
       "    T extends Object declared in method <T>asList(T...)\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 137\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 137\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:4: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 154\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 154\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:32: error: duplicate class: Main\n",
       "public class Main {\n",
       "       ^\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:32: error: duplicate class: Main\n",
       "public class Main {\n",
       "       ^\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Compilation failed! Idx: 162\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Compilation failed! Idx: 162\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">stderr: Main.java:6: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n",
       "</pre>\n"
      ],
      "text/plain": [
       "stderr: Main.java:6: error: class Solution is public, should be declared in a file named Solution.java\n",
       "public class Solution {\n",
       "       ^\n",
       "1 error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/playground/llama3_dir_a/',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', args, samples=164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4ebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a735291",
   "metadata": {},
   "source": [
    "## FT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d3107",
   "metadata": {},
   "source": [
    "### FT Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {'r': 16,  'alpha': 16, 'dropout': 0.00, 'lr': 1.0e-4, 'decay': 0.0},   # A\",\n",
    "    {'r': 64,  'alpha': 32, 'dropout': 0.10, 'lr': 8.0e-5, 'decay': 0.05},  # B\",\n",
    "    {'r': 32,  'alpha': 32, 'dropout': 0.05, 'lr': 5.0e-5, 'decay': 0.01},  # C\",\n",
    "    {'r': 8, 'alpha': 64, 'dropout': 0.00, 'lr': 1.5e-4, 'decay': 0.0},     # D\",\n",
    "    {'r': 128, 'alpha': 32, 'dropout': 0.10, 'lr': 7.0e-5 , 'decay': 0.02}, # E\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fe37e",
   "metadata": {},
   "source": [
    "### FineTuning Config #A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = configs[0]['r'], \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = configs[0]['alpha'],\n",
    "    lora_dropout = configs[0]['dropout'],\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=10000, test_size=500):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "def add_conversations(row):\n",
    "    preface = 'Read the following problem description. Fully implement a solution in Java. Your response should only contain the code, no explanations.\\n\\n'\n",
    "    problem_description = row['description']\n",
    "    code = row['code']\n",
    "\n",
    "    row['conversations'] = [\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': preface + problem_description,\n",
    "        },\n",
    "        {\n",
    "            'role': 'assistant', \n",
    "            'content': code,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return row\n",
    "\n",
    "def chat_format(row):\n",
    "    convos = row['conversations']\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { 'text' : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(my_dataset)\n",
    "\n",
    "train_set = train_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "test_set = test_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "\n",
    "train_set = train_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "test_set = test_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "\n",
    "train_set = train_set.map(chat_format, batched=True)\n",
    "test_set = test_set.map(chat_format, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df64713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir():\n",
    "    idx = 0\n",
    "    base_path = '/root/srf-project/playground/llama3-java-finetune-'\n",
    "    already_existing = True\n",
    "\n",
    "    while already_existing:\n",
    "        idx += 1\n",
    "        path = base_path + str(idx)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            already_existing = False\n",
    "            os.mkdir(path)\n",
    "            return path\n",
    "        \n",
    "output_path = get_save_dir()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    warmup_ratio = 0.05,\n",
    "    num_train_epochs = 1,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps = 0.125,\n",
    "    learning_rate = configs[0]['lr'],\n",
    "    bf16 = True,\n",
    "    logging_steps = 5,\n",
    "    logging_first_step=True,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = configs[0]['decay'],\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    save_steps = 0.2,\n",
    "    save_total_limit = 3,\n",
    "    output_dir = output_path,\n",
    "    report_to = \"wandb\", \n",
    "    run_name = 'quantized'\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_set,\n",
    "    eval_dataset = test_set,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 8,\n",
    "    args = training_args,\n",
    "    # packing = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats_a = trainer.train()\n",
    "wandb.finish()\n",
    "del model, tokenizer, train_set, test_set, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ae383",
   "metadata": {},
   "source": [
    "### FineTuning Config #B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66312846",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = configs[1]['r'], \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = configs[1]['alpha'],\n",
    "    lora_dropout = configs[1]['dropout'],\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=3000, test_size=200):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "def add_conversations(row):\n",
    "    preface = 'Read the following problem description. Fully implement a solution in Java. Your response should only contain the code, no explanations.\\n\\n'\n",
    "    problem_description = row['description']\n",
    "    code = row['code']\n",
    "\n",
    "    row['conversations'] = [\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': preface + problem_description,\n",
    "        },\n",
    "        {\n",
    "            'role': 'assistant', \n",
    "            'content': code,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return row\n",
    "\n",
    "def chat_format(row):\n",
    "    convos = row['conversations']\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { 'text' : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325dcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(my_dataset)\n",
    "\n",
    "train_set = train_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "test_set = test_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "\n",
    "train_set = train_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "test_set = test_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "\n",
    "train_set = train_set.map(chat_format, batched=True)\n",
    "test_set = test_set.map(chat_format, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir():\n",
    "    idx = 0\n",
    "    base_path = '/root/srf-project/playground/llama3-java-finetune-'\n",
    "    already_existing = True\n",
    "\n",
    "    while already_existing:\n",
    "        idx += 1\n",
    "        path = base_path + str(idx)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            already_existing = False\n",
    "            os.mkdir(path)\n",
    "            return path\n",
    "        \n",
    "output_path = get_save_dir()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    warmup_ratio = 0.05,\n",
    "    num_train_epochs = 1,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps = 0.125,\n",
    "    learning_rate = configs[1]['lr'],\n",
    "    bf16 = True,\n",
    "    logging_steps = 5,\n",
    "    logging_first_step=True,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = configs[1]['decay'],\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    save_steps = 0.2,\n",
    "    save_total_limit = 3,\n",
    "    output_dir = output_path,\n",
    "    report_to = \"wandb\", \n",
    "    run_name = 'quantized'\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_set,\n",
    "    eval_dataset = test_set,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 8,\n",
    "    args = training_args,\n",
    "    # packing = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff750602",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats_b = trainer.train()\n",
    "wandb.finish()\n",
    "del model, tokenizer, train_set, test_set, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38dc88",
   "metadata": {},
   "source": [
    "### FineTuning Config #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = configs[2]['r'], \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = configs[2]['alpha'],\n",
    "    lora_dropout = configs[2]['dropout'],\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0042acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=3000, test_size=200):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "def add_conversations(row):\n",
    "    preface = 'Read the following problem description. Fully implement a solution in Java. Your response should only contain the code, no explanations.\\n\\n'\n",
    "    problem_description = row['description']\n",
    "    code = row['code']\n",
    "\n",
    "    row['conversations'] = [\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': preface + problem_description,\n",
    "        },\n",
    "        {\n",
    "            'role': 'assistant', \n",
    "            'content': code,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return row\n",
    "\n",
    "def chat_format(row):\n",
    "    convos = row['conversations']\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { 'text' : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(my_dataset)\n",
    "\n",
    "train_set = train_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "test_set = test_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "\n",
    "train_set = train_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "test_set = test_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "\n",
    "train_set = train_set.map(chat_format, batched=True)\n",
    "test_set = test_set.map(chat_format, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir():\n",
    "    idx = 0\n",
    "    base_path = '/root/srf-project/playground/llama3-java-finetune-'\n",
    "    already_existing = True\n",
    "\n",
    "    while already_existing:\n",
    "        idx += 1\n",
    "        path = base_path + str(idx)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            already_existing = False\n",
    "            os.mkdir(path)\n",
    "            return path\n",
    "        \n",
    "output_path = get_save_dir()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    warmup_ratio = 0.05,\n",
    "    num_train_epochs = 1,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps = 0.125,\n",
    "    learning_rate = configs[2]['lr'],\n",
    "    bf16 = True,\n",
    "    logging_steps = 5,\n",
    "    logging_first_step=True,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = configs[2]['decay'],\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    save_steps = 0.2,\n",
    "    save_total_limit = 3,\n",
    "    output_dir = output_path,\n",
    "    report_to = \"wandb\", \n",
    "    run_name = 'quantized'\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_set,\n",
    "    eval_dataset = test_set,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 8,\n",
    "    args = training_args,\n",
    "    # packing = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a247d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats_c = trainer.train()\n",
    "wandb.finish()\n",
    "del model, tokenizer, train_set, test_set, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada5e2d",
   "metadata": {},
   "source": [
    "### Unsloth FT Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623de4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2884cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77567e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=10000, test_size=500):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "def add_conversations(row):\n",
    "    preface = 'Read the following problem description. Fully implement a solution in Java. Your response should only contain the code, no explanations.\\n\\n'\n",
    "    problem_description = row['description']\n",
    "    code = row['code']\n",
    "\n",
    "    row['conversations'] = [\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': preface + problem_description,\n",
    "        },\n",
    "        {\n",
    "            'role': 'assistant', \n",
    "            'content': code,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return row\n",
    "\n",
    "def chat_format(row):\n",
    "    convos = row['conversations']\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { 'text' : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d36480",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(my_dataset)\n",
    "\n",
    "train_set = train_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "test_set = test_set.map(add_description, remove_columns=['p_id', 'language', 'status'])\n",
    "\n",
    "train_set = train_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "test_set = test_set.map(add_conversations, remove_columns=['code', 'description'])\n",
    "\n",
    "train_set = train_set.map(chat_format, batched=True)\n",
    "test_set = test_set.map(chat_format, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e93f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir():\n",
    "    idx = 0\n",
    "    base_path = '/root/srf-project/playground/llama3-java-finetune-'\n",
    "    already_existing = True\n",
    "\n",
    "    while already_existing:\n",
    "        idx += 1\n",
    "        path = base_path + str(idx)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            already_existing = False\n",
    "            os.mkdir(path)\n",
    "            return path\n",
    "        \n",
    "output_path = get_save_dir()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    # auto_find_batch_size = True,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    warmup_ratio = 0.05,\n",
    "    num_train_epochs = 1,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps = 0.05,\n",
    "    learning_rate = 2e-4,\n",
    "    bf16 = True,\n",
    "    logging_steps = 5,\n",
    "    logging_first_step=True,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    save_steps = 0.2,\n",
    "    save_total_limit = 3,\n",
    "    output_dir = output_path,\n",
    "    report_to = \"wandb\", \n",
    "    run_name = 'quantized'\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_set,\n",
    "    eval_dataset = test_set,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 8,\n",
    "    args = training_args,\n",
    "    # packing = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c23ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5357b02",
   "metadata": {},
   "source": [
    "## Previous FT Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b3a1c",
   "metadata": {},
   "source": [
    "### Load Model / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be92a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=t.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token'''\n",
    "\n",
    "# model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct'\n",
    "model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/root/srf-project/data/codenet_questions/filtered_problem_descriptions.json\", \"r\") as f:\n",
    "    problem_descriptions = json.load(f)\n",
    "\n",
    "def get_filtered_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    dataset = dataset.filter(lambda x: x['p_id'] in problem_descriptions.keys())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_train_test(dataset, train_size=10000, test_size=500):\n",
    "    shuffled = dataset.shuffle(seed=47)\n",
    "    train_set = shuffled.select(range(train_size))\n",
    "    test_set = shuffled.select(range(train_size, train_size + test_size))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "filtered = get_filtered_dataset('Java')\n",
    "train_set, test_set = get_train_test(filtered, test_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_description(row):\n",
    "    description = problem_descriptions[row['p_id']]\n",
    "    row['description'] = description\n",
    "    return row\n",
    "\n",
    "train_set = train_set.map(add_description)\n",
    "test_set = test_set.map(add_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d91be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_final_prompt(row):\n",
    "    description = row['description'].strip()\n",
    "    code = row['code']\n",
    "\n",
    "    final_prompt = '\\n'.join([description, '<answer>', code, '</answer>'])\n",
    "    row['final_prompt'] = final_prompt\n",
    "\n",
    "    return row\n",
    "\n",
    "train_set = train_set.map(add_final_prompt)\n",
    "test_set = test_set.map(add_final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bde88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(record):\n",
    "    final_prompt = record['final_prompt']\n",
    "    msg = [\n",
    "        {'role': 'user', 'content': final_prompt}\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer.apply_chat_template(msg, add_generation_prompt=True, return_dict=True)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "final_train_set = train_set.map(tokenize, num_proc=32)\n",
    "final_test_set = test_set.map(tokenize, num_proc=32)\n",
    "\n",
    "final_train_set = final_train_set.select_columns(['input_ids', 'attention_mask'])\n",
    "final_test_set = final_test_set.select_columns(['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47594328",
   "metadata": {},
   "source": [
    "### FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.00,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e45159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_dir():\n",
    "    idx = 0\n",
    "    base_path = '/root/srf-project/playground/llama3-java-finetune-'\n",
    "    already_existing = True\n",
    "\n",
    "    while already_existing:\n",
    "        idx += 1\n",
    "        path = base_path + str(idx)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            already_existing = False\n",
    "            os.mkdir(path)\n",
    "            return path\n",
    "        \n",
    "output_path = get_save_dir()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path,\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=0.1,\n",
    "    # eval_on_start=True,\n",
    "    per_device_train_batch_size=1, \n",
    "    # auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # gradient_checkpointing=True,\n",
    "    torch_empty_cache_steps=1,\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,  \n",
    "    bf16=True,\n",
    "    save_steps=0.2,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=0.02,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='chat_template',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_train_set,\n",
    "    eval_dataset=final_test_set,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90465e",
   "metadata": {},
   "source": [
    "### RunEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bd49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.main import run_eval\n",
    "\n",
    "args = {\n",
    "    'model': 'hf/local',\n",
    "    'model_path': '/root/srf-project/playground/llama3-java-finetune-a/checkpoint-625',\n",
    "    'device': 'auto',\n",
    "    'torch_dtype': 'auto'\n",
    "}\n",
    "\n",
    "run_eval('java', args, samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ee715",
   "metadata": {},
   "source": [
    "## GPU Deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import gc\n",
    "\n",
    "free_memory, total_memory = t.cuda.mem_get_info()\n",
    "\n",
    "# Convert bytes to GB\n",
    "free_memory_gb = free_memory / (1024 * 1024 * 1024)\n",
    "total_memory_gb = total_memory / (1024 * 1024 * 1024)\n",
    "mem_used = t.cuda.device_memory_used() / (1024 ** 3)\n",
    "\n",
    "print(f\"Free GPU Memory: {free_memory_gb:.2f} GB\")\n",
    "print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "print(f'Memory Used: {mem_used:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e20428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

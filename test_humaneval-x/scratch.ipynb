{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import inspect_ai\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264e082",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d97a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl_all(filename: str):\n",
    "    results = []\n",
    "    fp = gzip.open(open(filename, \"rb\"), \"rt\")\n",
    "    for line in fp:\n",
    "        if any(not x.isspace() for x in line):\n",
    "            results.append(json.loads(line))\n",
    "    fp.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e98c1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_content = stream_jsonl_all('data/python_data.gz')\n",
    "cpp_content = stream_jsonl_all('data/cpp_data.gz')\n",
    "go_content = stream_jsonl_all('data/go_data.gz')\n",
    "java_content = stream_jsonl_all('data/java_data.gz')\n",
    "js_content = stream_jsonl_all('data/js_data.gz')\n",
    "content = [python_content, cpp_content, go_content, java_content, js_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f8c1e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = stream_jsonl_all('data/python_generations.gz')\n",
    "generations[0]['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f08897aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'import', 'docstring', 'declaration', 'canonical_solution', 'test', 'test_setup', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'text', 'declaration', 'example_test'])\n",
      "\n",
      "dict_keys(['task_id', 'prompt', 'canonical_solution', 'test', 'declaration', 'example_test'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in content:\n",
    "    print(lang[0].keys())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a2490",
   "metadata": {},
   "source": [
    "### LLM output => code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27679d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# got to a) look at *my* llm current output.\n",
    "# b) if that's the same as 'generation', great. Else figure processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44909f",
   "metadata": {},
   "source": [
    "#### Inspect output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "981a5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log = inspect_ai.log.read_eval_log('../logs/2025-07-09T22-22-28+00-00_humaneval_CQNY2ZDiYiHHuJB25hDdMS.eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc1284b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageAssistant(id='LMQXcVBZ5zCgTLK3TE4X7F', content='```python\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False\\n```', source='generate', metadata=None, internal=None, role='assistant', tool_calls=None, model='gpt-4o-mini-2024-07-18')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_log.samples[0].output.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7dab4036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(test_log.samples[0].output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb24c792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read the following function signature and docstring, and fully implement\n",
      "the function described. Your response should only contain the code for\n",
      "this function.\n",
      "\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_log.samples[0].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddf894",
   "metadata": {},
   "source": [
    "#### HumanEval-X output\n",
    "*now i have prompt template, feed it into model & look at output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "880f609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"\"\"\n",
    "Read the following function signature and docstring, and fully implement\n",
    "the function described. Your response should only contain the code for\n",
    "this function.\\n\n",
    "\"\"\"\n",
    "\n",
    "LANG_PREFIX = {\n",
    "    \"cpp\"          : \"// language: C++\",\n",
    "    \"java\"         : \"// language: Java\",\n",
    "    \"js\"           : \"// language: JavaScript\",\n",
    "    \"javascript\"   : \"// language: JavaScript\",\n",
    "    \"go\"           : \"// language: Go\",\n",
    "    \"python\"       : \"# language: Python\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c32ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lang = 'go'\n",
    "model_input = INSTRUCTION + LANG_PREFIX[my_lang] + '\\n' + go_content[0]['prompt'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  input=model_input\n",
    ")\n",
    "\n",
    "response_text = response.output[0].content[0].text\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71545e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', device_map='auto', torch_dtype=t.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', padding_side=\"left\", torch_dtype=t.bfloat16)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0f83d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA version: 12.6\n",
      "CUDA available: False\n",
      "CUDA device count: 1\n",
      "CUDA device name: N/A\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5ed0c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ec1e6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n"
     ]
    }
   ],
   "source": [
    "print(t.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c58de498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006, 128006, 128006, 128006, 128006, 128006, 128006, 128006,\n",
       "         128006, 128006, 128006, 128006, 128006, 128006, 128006, 128006, 128006,\n",
       "         128006, 128006, 128006]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate this from model now lol.\n",
    "# dont be pussy.\n",
    "# two things. one generate from openai model\n",
    "# two, generate from hf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8551c95",
   "metadata": {},
   "source": [
    "relevant?\n",
    "- task id \n",
    "- prompt\n",
    "- canonical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4428ece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'Python/0',\n",
       " 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n',\n",
       " 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n',\n",
       " 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(has_close_elements):\\n    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\ncheck(has_close_elements)\",\n",
       " 'text': '    Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True',\n",
       " 'declaration': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n',\n",
       " 'example_test': 'def check(has_close_elements):\\n    assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False\\n    assert has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) == True\\ncheck(has_close_elements)\\n'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e61e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(has_close_elements):\n",
      "    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
      "    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
      "    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
      "    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
      "    assert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
      "    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
      "    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
      "\n",
      "check(has_close_elements)\n"
     ]
    }
   ],
   "source": [
    "# testing, python stuff\n",
    "for sample in python_content:\n",
    "    task_id = sample['task_id']\n",
    "    language = task_id.split('/')[0].lower()\n",
    "\n",
    "    prompt = sample['prompt']\n",
    "    test = python_content[0]['test']\n",
    "    break\n",
    "    # code = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_humaneval_test(sample, problems):\n",
    "    task_id = sample[\"task_id\"]\n",
    "    language = task_id.split(\"/\")[0].lower()\n",
    "\n",
    "    prompt = sample[\"prompt\"]\n",
    "    test = problems[task_id][\"test\"]\n",
    "    code = sample[\"generation\"]\n",
    "\n",
    "    # Pre-process for different languages\n",
    "    if language == \"python\":\n",
    "        code_ = []\n",
    "        for line in code.split(\"\\n\"):\n",
    "            if (len(line.strip()) > 0 and line[0] != ' ' and line[0] != '\\t'):\n",
    "                break\n",
    "            code_.append(line)\n",
    "        code = \"\\n\".join(code_)\n",
    "        test_setup = \"\\n\".join(IMPORT_HELPER[\"python\"]) + \"\\n\"\n",
    "        test_string = test_setup + prompt + code + \"\\n\" + test + \"\\n\"\n",
    "    elif language == \"cpp\":\n",
    "        test_set_up = \"\"\n",
    "        for s in IMPORT_HELPER[\"cpp\"]:\n",
    "            if s not in prompt:\n",
    "                test_set_up += s + \"\\n\"\n",
    "        test_string = test_set_up + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "    elif language == \"java\":\n",
    "        test_string = prompt + code + \"\\n\" + test\n",
    "    elif language == \"js\" or language == \"javascript\":\n",
    "        test_string = prompt + code + \"\\n\" + test\n",
    "    elif language == \"go\":\n",
    "        import_string = problems[task_id][\"import\"]\n",
    "        prompt = prompt.replace(import_string, \"\")\n",
    "        test = problems[task_id][\"test\"]\n",
    "        test_setup = problems[task_id][\"test_setup\"]\n",
    "        other_pkgs = []\n",
    "        for pkg in IMPORT_HELPER[\"go\"]:\n",
    "            if pkg not in test_setup:\n",
    "                p = pkg.split(\"/\")[-1]\n",
    "                if p + \".\" in code:\n",
    "                    other_pkgs.append(f\"\\\"{pkg}\\\"\")\n",
    "        if other_pkgs:\n",
    "            import_other_pkgs = \"import (\\n\" + \"    \".join([p + \"\\n\" for p in other_pkgs]) + \")\"\n",
    "            test_string = test_setup + \"\\n\" + import_other_pkgs + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "        else:\n",
    "            test_string = test_setup + \"\\n\" + prompt + code + \"\\n\" + test\n",
    "\n",
    "    return test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_jsonl = python_content\n",
    "for sample in tqdm(sample_jsonl):\n",
    "    task_id = sample['task_id']\n",
    "    lang = task_id.split('/')[0].lower()\n",
    "    if lang == 'javascript': \n",
    "        lang = 'js'\n",
    "    \n",
    "    sample[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_functional_correctness(\n",
    "        input_file: str = None,\n",
    "        tmp_dir: str = \"./\",\n",
    "        n_workers: int = 32,\n",
    "        timeout: float = 500.0,\n",
    "        problem_file: str = \"../data/humaneval_python.jsonl.gz\",\n",
    "        out_dir: str = None,\n",
    "        k: List[int] = [1, 10, 100],\n",
    "):\n",
    "    \n",
    "    problems = read_dataset(problem_file,\n",
    "                            dataset_type=\"humaneval\")\n",
    "    sample_jsonl = stream_jsonl_all(input_file)\n",
    "\n",
    "    suffix = \"_results.jsonl\"\n",
    "    if out_dir is not None:\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_file = os.path.join(out_dir, input_file.split('/')[-1].replace(\".jsonl\", suffix))\n",
    "    else:\n",
    "        out_file = os.path.join(input_file.replace(\".jsonl\", suffix))\n",
    "\n",
    "    if \"/codegeex/benchmark/humaneval-x/\" in input_file:\n",
    "        test_groundtruth = True\n",
    "\n",
    "    if \"-to-\" in input_file:\n",
    "        translation_mode = True\n",
    "    else:\n",
    "        translation_mode = False\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "\n",
    "        futures = []\n",
    "        completion_id = Counter()\n",
    "        n_samples = 0\n",
    "        results = defaultdict(list)\n",
    "\n",
    "        print(\"Reading samples...\")\n",
    "        for sample in tqdm(sample_jsonl):\n",
    "            task_id = sample[\"task_id\"]\n",
    "            lang = task_id.split(\"/\")[0].lower()\n",
    "            if translation_mode:\n",
    "                task_id = sample[\"task_id\"].split(\"/\")[-1]\n",
    "                lang = regex.findall(\"-to-.*-\", input_file)[0].split(\"-to-\")[-1].rstrip(\"-\")\n",
    "                for l in LANGUAGE_NAME:\n",
    "                    if l in lang:\n",
    "                        lang = l\n",
    "                        break\n",
    "                task_id = f\"{LANGUAGE_NAME[lang]}/{task_id}\"\n",
    "            if lang == \"javascript\":\n",
    "                lang = \"js\"\n",
    "            tmp_dir_ = os.path.join(tmp_dir, lang, \"evaluation\")\n",
    "            sample[\"task_id\"] = task_id\n",
    "            sample[\"test_code\"] = process_humaneval_test(sample, problems, False)\n",
    "            if sample[\"test_code\"] is None:\n",
    "                continue\n",
    "            if \"completion_id\" in sample:\n",
    "                completion_id_ = sample[\"completion_id\"]\n",
    "            else:\n",
    "                completion_id_ = completion_id[task_id]\n",
    "            args = (task_id, sample, lang, timeout, tmp_dir_, completion_id_)\n",
    "            future = executor.submit(check_correctness, *args)\n",
    "            futures.append(future)\n",
    "            completion_id[task_id] += 1\n",
    "            n_samples += 1\n",
    "\n",
    "        print(completion_id)\n",
    "        if len(completion_id) == len(problems):\n",
    "            evaluate_pass_at_k = True\n",
    "        else:\n",
    "            evaluate_pass_at_k = False\n",
    "\n",
    "        print(\"Running test suites...\")\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            result = future.result()\n",
    "            results[result[\"task_id\"]].append((result[\"completion_id\"], result))\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate pass@k.\n",
    "    total, correct = [], []\n",
    "    for result in results.values():\n",
    "        passed = [r[1][\"passed\"] for r in result]\n",
    "        total.append(len(passed))\n",
    "        correct.append(sum(passed))\n",
    "    total = np.array(total)\n",
    "    correct = np.array(correct)\n",
    "    if evaluate_pass_at_k:\n",
    "        ks = k\n",
    "        pass_at_k = {f\"pass@{k}\": estimate_pass_at_k(total, correct, k).mean()\n",
    "                     for k in ks if (total >= k).all()}\n",
    "        print(pass_at_k)\n",
    "    else:\n",
    "        print(\"Total:\", np.sum(total))\n",
    "        print(\"Correct:\", np.sum(correct))\n",
    "\n",
    "    print(\"Writing to: \", out_file)\n",
    "    if out_file.endswith(\".gz\"):\n",
    "        fp = gzip.GzipFile(fileobj=open(out_file, \"wb\"), mode=\"wb\")\n",
    "        for res in results.values():\n",
    "            for r in res:\n",
    "                fp.write((json.dumps(r[1]) + \"\\n\").encode(\"utf-8\"))\n",
    "    else:\n",
    "        fp = open(out_file, 'w')\n",
    "        for res in results.values():\n",
    "            for r in res:\n",
    "                fp.write(json.dumps(r[1]) + \"\\n\")\n",
    "    fp.close()\n",
    "\n",
    "    print(\"Evaluation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b38c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL â€“ figure how humaneval-x generates / prompts model for code. compare w/ inspect.\n",
    "\n",
    "# with inspect, they\n",
    "#   a) prepend a default system prompt\n",
    "#   b) add the dataset['prompt']\n",
    "# \n",
    "# will this work for java, etc too? \n",
    "#   YA i think so. should be fine.\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38350e1",
   "metadata": {},
   "source": [
    "### Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl(filename: str):\n",
    "    \"\"\"\n",
    "    Parses each jsonl line and yields it as a dictionary\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as gzfp:\n",
    "        with gzip.open(gzfp, \"rt\") as fp:\n",
    "            for line in fp:\n",
    "                if any(not x.isspace() for x in line):\n",
    "                    yield json.loads(line)\n",
    "\n",
    "def read_dataset(data_file: str = None):\n",
    "    dataset = {task[\"task_id\"]: task for task in stream_jsonl(data_file)}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c69630",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/humaneval_python.jsonl.gz'\n",
    "out = read_dataset(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

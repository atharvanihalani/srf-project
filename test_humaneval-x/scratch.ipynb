{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, AutoModelForSequenceClassification, DataCollatorForLanguageModeling\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "import asyncio\n",
    "import os\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import bitsandbytes\n",
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import wandb\n",
    "import einops\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a735291",
   "metadata": {},
   "source": [
    "## FT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c133",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c037c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_java = load_dataset('iNeil77/CodeNet', 'Java', split='train')\n",
    "dataset_java = dataset_java.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "dataset_java = dataset_java.filter(lambda x: x['status']=='Accepted')\n",
    "# dataset_java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b899951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_js = load_dataset('iNeil77/CodeNet', 'JavaScript', split='train')\n",
    "# dataset_js = dataset_js.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "# dataset_js = dataset_js.filter(lambda x: x['status']=='Accepted')\n",
    "# # dataset_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae57ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772b6bbde07944929c2c4b35d1f89622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c522dfde1d498dae544a0e6e69665f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset_cpp = load_dataset('iNeil77/CodeNet', 'C++', split='train')\n",
    "# dataset_cpp = dataset_cpp.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "# dataset_cpp = dataset_cpp.filter(lambda x: x['status']=='Accepted')\n",
    "# # dataset_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74364800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_go = load_dataset('iNeil77/CodeNet', 'Go', split='train')\n",
    "# dataset_go = dataset_go.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "# dataset_go = dataset_go.filter(lambda x: x['status']=='Accepted')\n",
    "# # dataset_go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_python = load_dataset('iNeil77/CodeNet', 'Python', split='train')\n",
    "# dataset_python = dataset_python.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "# dataset_python = dataset_python.filter(lambda x: x['status']=='Accepted')\n",
    "# # dataset_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <iostream>\n",
      "#include<bits/stdc++.h>\n",
      "using namespace std;\n",
      "typedef long long ll;\n",
      "ll dp[51],n,a,b,L,ok;\n",
      "vector<ll> v[31];\n",
      "int main(void){\n",
      "    while(1){\n",
      "    cin>>n;\n",
      "    //cout<<n<<endl;\n",
      "    if(n==0)return 0;\n",
      "    ok=1;\n",
      "    for(int i=0;i<31;i++)v[i]={};\n",
      "    for(int i=0;i<n;i++){\n",
      "        dp[i]=(1LL<<i);\n",
      "    }\n",
      "    for(int i=0;i<n;i++){\n",
      "        cin>>a;\n",
      "        for(int j=0;j<a;j++){\n",
      "            cin>>b;\n",
      "            v[b-1].push_back(i);\n",
      "        }\n",
      "    }\n",
      "    for(int i=0;i<31;i++){\n",
      "        L=0;\n",
      "        for(auto x:v[i]){\n",
      "            L|=dp[x];\n",
      "        }\n",
      "        if(L==(1LL<<n)-1){\n",
      "            cout<<i+1<<endl;\n",
      "            ok=0;\n",
      "            break;\n",
      "        }\n",
      "        for(auto x:v[i]){\n",
      "            dp[x]=L;\n",
      "        }\n",
      "    }\n",
      "    if(ok)cout<<-1<<endl;\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "--x---x---x--\n",
      "\n",
      "#include<ctime>\n",
      "#include<cmath>\n",
      "#include<cstdio>\n",
      "#include<cstring>\n",
      "#include<iostream>\n",
      "#include<algorithm>\n",
      "#include<queue>\n",
      "#include<vector>\n",
      "#define p 1000000007\n",
      "#define inv2 500000004\n",
      "#define file(x)freopen(x\".in\",\"r\",stdin);freopen(x\".out\",\"w\",stdout)\n",
      "#define rt register int\n",
      "#define l putchar('\\n')\n",
      "#define ll long long\n",
      "#define r read()\n",
      "using namespace std;\n",
      "inline ll read(){\n",
      "    ll x=0;char zf=1;char ch=getchar();\n",
      "    while(ch!='-'&&!isdigit(ch))ch=getchar();\n",
      "    if(ch=='-')zf=-1,ch=getchar();\n",
      "    while(isdigit(ch))x=x*10+ch-'0',ch=getchar();return x*zf;\n",
      "}\n",
      "void write(ll y){if(y<0)putchar('-'),y=-y;if(y>9)write(y/10);putchar(y%10+48);}\n",
      "void writeln(const ll y){write(y);putchar('\\n');}\n",
      "int k,m,n,x,y,z,cnt,ans;\n",
      "int f[3010][3010],g[3010][3010],a[3010];\n",
      "int main(){\n",
      "\tn=r;m=r;\n",
      "\tfor(rt i=1;i<=n;i++)a[i]=r;\n",
      "\tfor(rt i=1;i<=n;i++)\n",
      "\tfor(rt j=1;j<=n;j++)if(a[i]>a[j])f[i][j]=1;\n",
      "\tfor(rt i=1;i<=m;i++){\n",
      "\t\tx=r;y=r;\n",
      "\t\tfor(rt j=1;j<=n;j++)g[x][j]=f[x][j],g[j][x]=f[j][x],g[y][j]=f[y][j],g[j][y]=f[j][y];\n",
      "\t\tf[x][y]=f[y][x]=1ll*(g[x][y]+g[y][x])*inv2%p;\n",
      "\t\tfor(rt j=1;j<=n;j++){\n",
      "\t\t\tconst int v1=1ll*(g[x][j]+g[y][j])*inv2%p,v2=1ll*(g[j][x]+g[j][y])*inv2%p;\n",
      "\t\t\tif(j!=y&&j!=x)f[x][j]=v1,f[j][x]=v2;\n",
      "\t\t\tif(j!=x&&j!=y)f[y][j]=v1,f[j][y]=v2;\n",
      "\t\t}\t\n",
      "\t}\n",
      "\n",
      "\tint ans=0;\n",
      "\tfor(rt i=1;i<n;i++)\n",
      "\tfor(rt j=i+1;j<=n;j++)(ans+=f[i][j])%=p;\n",
      "\tfor(rt i=1;i<=m;i++)ans=ans*2%p;\n",
      "\tcout<<ans;\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "--x---x---x--\n",
      "\n",
      "#include <iostream>\n",
      "#include <algorithm>\n",
      "#include <string>\n",
      "#include <vector>\n",
      "typedef long long ll;\n",
      "using namespace std;\n",
      "\n",
      "int main() {\n",
      "    const int inf = 1e+9;\n",
      "    int n, m;\n",
      "    cin >> n >> m;\n",
      "\n",
      "    int a[1000], b[1000], c[1000], dist[100][100];\n",
      "    for (int i = 0; i < n; i++) {\n",
      "        for (int j = 0; j < n; j++) {\n",
      "            if (i==j) dist[i][j] = 0;\n",
      "            else dist[i][j] = inf;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    for (int i = 0; i < m; i++) {\n",
      "        cin >> a[i] >> b[i] >> c[i];\n",
      "        a[i]--, b[i]--;\n",
      "        dist[a[i]][b[i]] = c[i];\n",
      "        dist[b[i]][a[i]] = c[i];\n",
      "    }\n",
      "\n",
      "    for (int k = 0; k < n; k++) {\n",
      "        for (int i = 0; i < n; i++) {\n",
      "            for (int j = 0; j < n; j++) {\n",
      "                dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j]);\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "\n",
      "    int ans = m;\n",
      "    for (int i = 0; i < m; i++) {\n",
      "        bool shortest = false;\n",
      "        for (int j = 0; j < n; j++) {\n",
      "            if (dist[j][a[i]] + c[i] == dist[j][b[i]]) {\n",
      "                shortest = true;\n",
      "            }\n",
      "        }\n",
      "        if (shortest) {\n",
      "            ans--;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    cout << ans << endl;\n",
      "    return 0;\n",
      "}\n",
      "--x---x---x--\n",
      "\n",
      "#include <bits/stdc++.h>\n",
      "using namespace std;\n",
      "#define int long long\n",
      "#define ms(x, t) memset(x, t, sizeof(x))\n",
      "#define ff first\n",
      "#define ss second\n",
      "#define all(ar) ar.begin(), ar.end()\n",
      "#define rall(ar) ar.rbegin(), ar.rend()\n",
      "#define mp make_pair\n",
      "#define pb push_back\n",
      "#define pf push_front\n",
      "#define endl \"\\n\"\n",
      "typedef pair<int, int> pll;\n",
      "typedef pair<int, pll> plll;\n",
      "typedef vector<int> vl;\n",
      "typedef vector<vector<int>> vvl;\n",
      "typedef vector<pll> vpll;\n",
      "#define trace1(a) cerr << #a << \": \" << a << endl;\n",
      "#define trace2(a, b) cerr << #a << \": \" << a << \" \" << #b << \": \" << b << endl;\n",
      "\n",
      "const int limit = 1e5 + 1;\n",
      "const int inf = 1e7;\n",
      "const int mod = 1e9 + 7;\n",
      "\n",
      "int32_t main() {\n",
      "  ios_base::sync_with_stdio(false);\n",
      "  cin.tie(NULL);\n",
      "  cout.tie(NULL);\n",
      "  string s;\n",
      "  cin >> s;\n",
      "  int n = s.size();\n",
      "  int ans = 0;\n",
      "  for (int i = 0; i < n / 2; i++) {\n",
      "    if (s[i] != s[n - i - 1]) {\n",
      "      ans++;\n",
      "    }\n",
      "  }\n",
      "  cout << ans << endl;\n",
      "  return 0;\n",
      "}\n",
      "--x---x---x--\n",
      "\n",
      "#include \"iostream\"\n",
      "#include \"climits\"\n",
      "#include \"list\"\n",
      "#include \"queue\"\n",
      "#include \"stack\"\n",
      "#include \"set\"\n",
      "#include \"functional\"\n",
      "#include \"algorithm\"\n",
      "#include \"string\"\n",
      "#include \"map\"\n",
      "#include \"unordered_map\"\n",
      "#include \"unordered_set\"\n",
      "#include \"iomanip\"\n",
      "#include \"cmath\"\n",
      "#include \"random\"\n",
      "#include \"bitset\"\n",
      "#include \"cstdio\"\n",
      "#include \"numeric\"\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "const long long int MOD = 1000000007;\n",
      "//const int MOD = 998244353;\n",
      "\n",
      "long long int N, M, K, H, W, L, R;\n",
      "//int N, M, K, H, W, L, R;\n",
      "\n",
      "\n",
      "\n",
      "int main() {\n",
      "\tios::sync_with_stdio(false);\n",
      "\tcin.tie(0);\n",
      "\n",
      "\tcin >> N >> M >> K;\n",
      "\tif (N > M)swap(N, M);\n",
      "\tif (N > K)swap(N, K);\n",
      "\tif (M > K)swap(K, M);\n",
      "\tif (N % 2 && M % 2 && K % 2) {\n",
      "\t\tcout << N * M << endl;\n",
      "\t\treturn 0;\n",
      "\t}\n",
      "\tcout << 0 << endl;\n",
      "}\n",
      "\n",
      "--x---x---x--\n",
      "\n",
      "#include <iostream>\n",
      "using namespace std;\n",
      "\n",
      "const int LEN = 100000;\n",
      "\n",
      "class pp {\n",
      "public:\n",
      "\tchar name[100];\n",
      "\tint t;\n",
      "};\n",
      "\n",
      "pp Q[LEN];\n",
      "int head, tail, n;\n",
      "\n",
      "void enqueue(pp x) {\n",
      "\tQ[tail] = x;\n",
      "\ttail = (tail + 1) % LEN;\n",
      "}\n",
      "\n",
      "pp dequeue() {\n",
      "\tpp x = Q[head];\n",
      "\thead = (head + 1) % LEN;\n",
      "\treturn x;\n",
      "}\n",
      "\n",
      "int min(int a, int b) {\n",
      "\treturn (a < b) ? a : b;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "\tint elaps = 0, c;\n",
      "\tint q;\n",
      "\tpp u;\n",
      "\t\n",
      "\tcin >> n >> q;\n",
      "\t\n",
      "\tfor(int i = 1; i <= n; i++) {\n",
      "\t\tcin >> Q[i].name;\n",
      "\t\tcin >> Q[i].t;\n",
      "\t}\n",
      "\n",
      "\thead = 1; tail = n + 1;\n",
      "\n",
      "\twhile(head != tail) {\n",
      "\t\tu = dequeue();\n",
      "\t\tc = min(q, u.t);\n",
      "\t\tu.t -= c;\n",
      "\t\telaps += c;\n",
      "\t\tif(u.t > 0) {\n",
      "\t\t\tenqueue(u);\n",
      "\t\t} else {\n",
      "\t\t\tcout << u.name << \" \" << elaps << endl;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "--x---x---x--\n",
      "\n",
      "#include <iostream>\n",
      "using namespace std;\n",
      "\n",
      "int main(){\n",
      "    int a,b,c,d;\n",
      "    cin >> a>>b>>c>>d;\n",
      "    int ans=0;\n",
      "    cout << max(0,min(b,d)-max(a,c)) <<endl;\n",
      "    return 0;\n",
      "}\n",
      "--x---x---x--\n",
      "\n",
      "#include <iostream>\n",
      "#include <cstdio>\n",
      "#include <cstdlib>\n",
      "#include <cstring>\n",
      "#include <algorithm>\n",
      "#include <string>\n",
      "#include <sstream>\n",
      "#include <complex>\n",
      "#include <vector>\n",
      "#include <list>\n",
      "#include <queue>\n",
      "#include <deque>\n",
      "#include <stack>\n",
      "#include <map>\n",
      "#include <set>\n",
      "#include <numeric>\n",
      "using namespace std;\n",
      "typedef long long int ll;\n",
      "\n",
      "#define EPS (1e-7)\n",
      "#define INF 1e18\n",
      "#define max(p,q)((p)>(q)?(p):(q))\n",
      "#define min(p,q)((p)<(q)?(p):(q))\n",
      "#define PI (acos(-1))\n",
      "\n",
      "#define REP(i, n) for(ll i = 0; i < (ll)(n); i++)\n",
      "ll MOD = pow(10, 9) + 7;\n",
      "\n",
      "int getX(ll num, int d){\n",
      "    int tmp = num >> d;\n",
      "    return (tmp % 2);\n",
      "}\n",
      "ll powmod(ll a, ll p, ll MOD) {\n",
      "\tll ans = 1;\n",
      "\tll mul = a;\n",
      "\tfor (; p > 0; p >>= 1, mul = (mul * mul) % MOD) {\n",
      "\t\tif ((p & 1) == 1)ans = (ans * mul) % MOD;\n",
      "\t}\n",
      "\treturn ans;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "\n",
      "    ll N;\n",
      "    cin >> N;\n",
      "    vector<ll> A(N);\n",
      "    REP(i, N) cin >> A[i];\n",
      "    ll ans = 0;\n",
      "    REP(i, 60){\n",
      "        ll tmp0 = 0, tmp1 = 0;\n",
      "        for(auto iter : A){\n",
      "            int tmp = getX(iter, i);\n",
      "            tmp == 0 ? tmp0++ : tmp1++;\n",
      "        }\n",
      "        ll x = powmod(2, i, MOD);\n",
      "        x = (x * tmp0) % MOD;\n",
      "        x = (x * tmp1) % MOD;\n",
      "        ans = (ans + x) % MOD;\n",
      "    }\n",
      "    cout << ans << endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "--x---x---x--\n",
      "\n",
      "#include<stdio.h>\n",
      "\n",
      "int n, A[20];\n",
      "\n",
      "bool exhaustiveSearch(int i, int m) {\n",
      "  if (m < 0 || i >= n) return false;\n",
      "  if (m == A[i]) return true;\n",
      "  else return exhaustiveSearch(i + 1, m) || exhaustiveSearch(i + 1, m - A[i]);\n",
      "}\n",
      "\n",
      "int main() {\n",
      "  int q, m;\n",
      "  \n",
      "  scanf(\"%d\", &n);\n",
      "  for ( int i = 0; i < n; i++ ) scanf(\"%d\", &A[i]);\n",
      "\n",
      "  scanf(\"%d\", &q);\n",
      "  for ( int i = 0; i < q; i++ ) {\n",
      "    scanf(\"%d\", &m);\n",
      "    exhaustiveSearch(0, m) ? printf(\"yes\\n\") : printf(\"no\\n\");\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "--x---x---x--\n",
      "\n",
      "#include<iostream>\n",
      "#include<stdio.h>\n",
      "using namespace std;\n",
      "int main(){\n",
      "\t\n",
      "\tint A,B;\n",
      "\tchar op;\n",
      "\tcin>>A>>op>>B;\n",
      "\tswitch(op)\n",
      "\t{\n",
      "\t\tcase'+':\n",
      "\t\tcout<<A+B;\n",
      "\t\tbreak;\n",
      "\t\tcase'-':\n",
      "\t\tcout<<A-B;\n",
      "\t\tbreak;\n",
      "\t\n",
      "\t}\n",
      "\t\n",
      "\t\treturn 0;\n",
      "}\n",
      "--x---x---x--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "#     idx = random.randint(0, dataset_cpp.num_rows)\n",
    "#     code = dataset_cpp[idx]['code']\n",
    "#     print(code)\n",
    "#     print('--x---x---x--\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9815e22",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f64568b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af484f26fdff41e0bdb10db108f8075d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07bde88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbef5f3b1a7d4ec3b21075fbd9f5f7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/348362 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(record):\n",
    "    code = record['code']\n",
    "    tokens = tokenizer(\n",
    "        code, \n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset_java = dataset_java.map(tokenize, batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdb49934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['p_id', 'language', 'status', 'code', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 348362\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dfa152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Adjust for Llama 3.1 if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "031387f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7370/2101947803.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    # per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,  \n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_java,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c31a9816",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 132.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 52.25 MiB is free. Process 3286969 has 47.47 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 308.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:2237\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2235\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   2238\u001b[39m         args=args,\n\u001b[32m   2239\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m   2240\u001b[39m         trial=trial,\n\u001b[32m   2241\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   2242\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:2578\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2571\u001b[39m context = (\n\u001b[32m   2572\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2573\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2574\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2575\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2576\u001b[39m )\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2581\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2582\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2583\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2584\u001b[39m ):\n\u001b[32m   2585\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2586\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:3792\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3791\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3792\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n\u001b[32m   3794\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3796\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3797\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3798\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/trainer.py:3879\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3877\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3878\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3879\u001b[39m outputs = model(**inputs)\n\u001b[32m   3880\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3881\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3882\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m.model_forward(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/peft/peft_model.py:1845\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1843\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1844\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1845\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_model(\n\u001b[32m   1846\u001b[39m             input_ids=input_ids,\n\u001b[32m   1847\u001b[39m             attention_mask=attention_mask,\n\u001b[32m   1848\u001b[39m             inputs_embeds=inputs_embeds,\n\u001b[32m   1849\u001b[39m             labels=labels,\n\u001b[32m   1850\u001b[39m             output_attentions=output_attentions,\n\u001b[32m   1851\u001b[39m             output_hidden_states=output_hidden_states,\n\u001b[32m   1852\u001b[39m             return_dict=return_dict,\n\u001b[32m   1853\u001b[39m             **kwargs,\n\u001b[32m   1854\u001b[39m         )\n\u001b[32m   1856\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1857\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1858\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:216\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.forward(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/utils/generic.py:961\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    960\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    963\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:460\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    442\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    443\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    445\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    458\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    461\u001b[39m         input_ids=input_ids,\n\u001b[32m    462\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    463\u001b[39m         position_ids=position_ids,\n\u001b[32m    464\u001b[39m         past_key_values=past_key_values,\n\u001b[32m    465\u001b[39m         inputs_embeds=inputs_embeds,\n\u001b[32m    466\u001b[39m         use_cache=use_cache,\n\u001b[32m    467\u001b[39m         cache_position=cache_position,\n\u001b[32m    468\u001b[39m         **kwargs,\n\u001b[32m    469\u001b[39m     )\n\u001b[32m    471\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/utils/generic.py:1069\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1067\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1070\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:390\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     hidden_states = decoder_layer(\n\u001b[32m    391\u001b[39m         hidden_states,\n\u001b[32m    392\u001b[39m         attention_mask=causal_mask,\n\u001b[32m    393\u001b[39m         position_ids=position_ids,\n\u001b[32m    394\u001b[39m         past_key_value=past_key_values,\n\u001b[32m    395\u001b[39m         cache_position=cache_position,\n\u001b[32m    396\u001b[39m         position_embeddings=position_embeddings,\n\u001b[32m    397\u001b[39m         **kwargs,\n\u001b[32m    398\u001b[39m     )\n\u001b[32m    400\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    402\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    403\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    404\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:304\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m residual = hidden_states\n\u001b[32m    303\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    305\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:152\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/srf-env/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 132.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 52.25 MiB is free. Process 3286969 has 47.47 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 308.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33489c8",
   "metadata": {},
   "source": [
    "## Un-Focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa865bb",
   "metadata": {},
   "source": [
    "### HuggingFace Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"karpathy/tiny_shakespeare\")\n",
    "print(dataset)\n",
    "# print('hlelow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe78681",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_size='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # Optional: saves memory\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311769d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'What is 5+5?', \n",
    "    'Tell me a poem.',\n",
    "    'Tell me a story.',\n",
    "]\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly, helpful chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": ''},\n",
    "]\n",
    "\n",
    "def qualitative_eval():\n",
    "    out = []\n",
    "    for question in questions:\n",
    "        message = message_template\n",
    "        message[1]['content'] = question\n",
    "\n",
    "        model_inputs = tokenizer.apply_chat_template(message, add_generation_prompt=True, return_tensors='pt').to('cuda')\n",
    "        generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n",
    "        text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split('assistant')[-1]\n",
    "\n",
    "        out.append(text)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfebac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19724db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-shakespeare\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./llama3-shakespeare\")\n",
    "tokenizer.save_pretrained(\"./llama3-shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cee218",
   "metadata": {},
   "source": [
    "### HuggingFace Training (0/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "datasets['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440dac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"]).to('cuda')\n",
    "\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = tokenizer.model_max_length\n",
    "\n",
    "first = True\n",
    "my_result = ...\n",
    "counter = 0\n",
    "\n",
    "def group_texts(examples):\n",
    "    global my_result, counter, first\n",
    "\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    counter += 1\n",
    "    if first:\n",
    "        my_result = result\n",
    "        first = False\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    # num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ca80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, device_map='auto', torch_dtype='auto')\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf5dbb",
   "metadata": {},
   "source": [
    "### HuggingFace Training (2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype='auto')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'What is 5+5?', \n",
    "    'Tell me a poem.',\n",
    "    'Tell me a story.',\n",
    "]\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly, helpful chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": ''},\n",
    "]\n",
    "\n",
    "def qualitative_eval():\n",
    "    out = []\n",
    "    for question in questions:\n",
    "        message = message_template\n",
    "        message[1]['content'] = question\n",
    "\n",
    "        model_inputs = tokenizer.apply_chat_template(message, add_generation_prompt=True, return_tensors='pt').to('cuda')\n",
    "        generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n",
    "        text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split('assistant')[-1]\n",
    "\n",
    "        out.append(text)\n",
    "    \n",
    "    return out\n",
    "\n",
    "out = qualitative_eval()\n",
    "# for sentence in out:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571345da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer(text['text'])\n",
    "\n",
    "dataset = load_dataset('karpathy/tiny_shakespeare', trust_remote_code=True)\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1028\n",
    "\n",
    "def group_text(input):\n",
    "    length = len(input['input_ids'])\n",
    "    n_batches = length // block_size\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    for k, t in input.items():\n",
    "        t = np.array(t[:n_batches * block_size])\n",
    "        t = einops.rearrange(t, '(bat block) -> bat block', bat=n_batches)\n",
    "        t = t.tolist()\n",
    "\n",
    "        result[k] = t\n",
    "\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"finetuned-tiny-shakespeare\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67594927",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a19e6",
   "metadata": {},
   "source": [
    "### HuggingFace Training (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb524c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', padding_side='left')\n",
    "# model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', device_map='auto', torch_dtype='auto')\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25135ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataset = load_dataset('yelp_review_full')\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
    "\n",
    "def tokenize(examples):\n",
    "    return ft_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True).to('cuda')\n",
    "dataset = ft_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b281df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"yelp_review_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval = dataset[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf08ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66b65b",
   "metadata": {},
   "source": [
    "### HuggingFace Practice  \n",
    "*getting comfortable with HF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        'meta-llama/Llama-3.1-8B-Instruct', \n",
    "        device_map = 'auto',\n",
    "        torch_dtype = 'auto',\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'Jack n jill went up the hill to smoke a lot of pot',\n",
    "    'Beautiful world, where are you?',\n",
    "    'In the beginning, the universe was created. This was widely regarded as a bad move.',\n",
    "]\n",
    "\n",
    "out = tokenizer(sentences, padding=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "\n",
    "out2 = tokenizer.decode(out[0])\n",
    "\n",
    "out3 = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "print(out2 + '\\n')\n",
    "print(out)\n",
    "\n",
    "out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4faf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"What is 5+5? Reply with a single word.\"\n",
    "model_inputs = tokenizer(test_sentence, return_tensors='pt').to('cuda')\n",
    "\n",
    "generated = model.generate(**model_inputs, max_new_tokens=100, temperature=0.3, repetition_penalty= 2.0)\n",
    "out = tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47eaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly, helpful chatbot.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is 5+5? Answer with a single word.\"},\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n",
    "print(tokenizer.batch_decode(generated_ids)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb4c3c",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97028351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda333b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LoRA configuration object\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # type of task to train on\n",
    "    inference_mode=False, # set to False for training\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e871434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(lora_config, adapter_name=\"lora_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9b376",
   "metadata": {},
   "source": [
    "### Async Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f066329",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def task1():\n",
    "    print(\"Task 1: Start\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Task 1: End\")\n",
    "\n",
    "async def task2():\n",
    "    print(\"Task 2: Start\")\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"Task 2: End\")\n",
    "\n",
    "# async def main():\n",
    "await asyncio.gather(task1(), task2())\n",
    "\n",
    "# await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_async():\n",
    "    prompt = 'hello world'\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    response = await client.responses.create(\n",
    "        model = 'gpt-4.1-mini',\n",
    "        input = prompt,\n",
    "    )\n",
    "\n",
    "    text_out = response.output[-1].content[0].text\n",
    "    return text_out\n",
    "\n",
    "def chat_reg():\n",
    "    prompt = 'hello world'\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4.1-mini',\n",
    "        input = prompt,\n",
    "    )\n",
    "\n",
    "    text_out = response.output[-1].content[0].text\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d13db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat_async()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

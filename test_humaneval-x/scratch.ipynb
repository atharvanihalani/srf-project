{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c1567d",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4417b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, AutoModelForSequenceClassification, DataCollatorForLanguageModeling\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import torch as t\n",
    "import asyncio\n",
    "import os\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import bitsandbytes\n",
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import wandb\n",
    "import einops\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "login(token = os.environ['HF_TOKEN'])\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a735291",
   "metadata": {},
   "source": [
    "## FT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c133",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(lang):\n",
    "    dataset = load_dataset('iNeil77/CodeNet', lang, split='train')\n",
    "    dataset = dataset.select_columns(['p_id', 'language', 'status', 'code'])\n",
    "    dataset = dataset.filter(lambda x: x['status']=='Accepted')\n",
    "    return dataset\n",
    "\n",
    "my_dataset = get_dataset('Java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     idx = random.randint(0, dataset_cpp.num_rows)\n",
    "#     code = dataset_cpp[idx]['code']\n",
    "#     print(code)\n",
    "#     print('--x---x---x--\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = wandb.Api()\n",
    "# run = api.run(\"atharva_nihalani-brown-university/huggingface/diw9fexc\")\n",
    "# metrics_dataframe = run.history()\n",
    "# # metrics_dataframe.to_csv(\"metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the GPU metrics\n",
    "# gpu_memory_util = metrics_dataframe.get(\"gpu.0.memory\")  # For the first GPU\n",
    "# gpu_memory_alloc = metrics_dataframe.get(\"gpu.0.memoryAllocated\")\n",
    "# print(gpu_memory_util)\n",
    "# print(gpu_memory_alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41a11c",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64568b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, pad_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\",\n",
    "    # load_in_4bit=True, \n",
    "    # bnb_4bit_compute_dtype=t.bfloat16,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bde88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(record):\n",
    "    code = record['code']\n",
    "    tokens = tokenizer(\n",
    "        code, \n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = my_dataset.map(tokenize, batched=True, num_proc=32)\n",
    "tokenized_dataset = tokenized_dataset.select_columns(['input_ids', 'attention_mask'])\n",
    "\n",
    "with open('seq_lengths.json', 'r') as f:\n",
    "    seq_lengths = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Adjust for Llama 3.1 if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031387f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-java-finetune\",\n",
    "    per_device_train_batch_size=8,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_persistent_workers=True,\n",
    "    num_train_epochs=1,  \n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_steps=0.05,\n",
    "    logging_steps=0.0005,\n",
    "    report_to=\"wandb\",\n",
    "    logging_first_step=True,\n",
    "    run_name='batch_auto_steps_1_b',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2d234",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ee715",
   "metadata": {},
   "source": [
    "### GPU Deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import gc\n",
    "\n",
    "free_memory, total_memory = t.cuda.mem_get_info()\n",
    "\n",
    "# Convert bytes to GB\n",
    "free_memory_gb = free_memory / (1024 * 1024 * 1024)\n",
    "total_memory_gb = total_memory / (1024 * 1024 * 1024)\n",
    "mem_used = t.cuda.device_memory_used() / (1024 ** 3)\n",
    "\n",
    "print(f\"Free GPU Memory: {free_memory_gb:.2f} GB\")\n",
    "print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "print(f'Memory Used: {mem_used:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e20428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.cuda.memory_allocated() / 1024**2, \"MB allocated\")\n",
    "print(t.cuda.memory_reserved() / 1024**2, \"MB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33489c8",
   "metadata": {},
   "source": [
    "## Un-Focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa865bb",
   "metadata": {},
   "source": [
    "### HuggingFace Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"karpathy/tiny_shakespeare\")\n",
    "print(dataset)\n",
    "# print('hlelow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe78681",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_size='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # Optional: saves memory\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311769d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'What is 5+5?', \n",
    "    'Tell me a poem.',\n",
    "    'Tell me a story.',\n",
    "]\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly, helpful chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": ''},\n",
    "]\n",
    "\n",
    "def qualitative_eval():\n",
    "    out = []\n",
    "    for question in questions:\n",
    "        message = message_template\n",
    "        message[1]['content'] = question\n",
    "\n",
    "        model_inputs = tokenizer.apply_chat_template(message, add_generation_prompt=True, return_tensors='pt').to('cuda')\n",
    "        generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n",
    "        text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split('assistant')[-1]\n",
    "\n",
    "        out.append(text)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfebac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated[\"input_ids\"])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19724db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-shakespeare\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./llama3-shakespeare\")\n",
    "tokenizer.save_pretrained(\"./llama3-shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cee218",
   "metadata": {},
   "source": [
    "### HuggingFace Training (0/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "datasets['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440dac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"]).to('cuda')\n",
    "\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = tokenizer.model_max_length\n",
    "\n",
    "first = True\n",
    "my_result = ...\n",
    "counter = 0\n",
    "\n",
    "def group_texts(examples):\n",
    "    global my_result, counter, first\n",
    "\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    counter += 1\n",
    "    if first:\n",
    "        my_result = result\n",
    "        first = False\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    # num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ca80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, device_map='auto', torch_dtype='auto')\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf5dbb",
   "metadata": {},
   "source": [
    "### HuggingFace Training (2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype='auto')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'What is 5+5?', \n",
    "    'Tell me a poem.',\n",
    "    'Tell me a story.',\n",
    "]\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly, helpful chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": ''},\n",
    "]\n",
    "\n",
    "def qualitative_eval():\n",
    "    out = []\n",
    "    for question in questions:\n",
    "        message = message_template\n",
    "        message[1]['content'] = question\n",
    "\n",
    "        model_inputs = tokenizer.apply_chat_template(message, add_generation_prompt=True, return_tensors='pt').to('cuda')\n",
    "        generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n",
    "        text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split('assistant')[-1]\n",
    "\n",
    "        out.append(text)\n",
    "    \n",
    "    return out\n",
    "\n",
    "out = qualitative_eval()\n",
    "# for sentence in out:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571345da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer(text['text'])\n",
    "\n",
    "dataset = load_dataset('karpathy/tiny_shakespeare', trust_remote_code=True)\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1028\n",
    "\n",
    "def group_text(input):\n",
    "    length = len(input['input_ids'])\n",
    "    n_batches = length // block_size\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    for k, t in input.items():\n",
    "        t = np.array(t[:n_batches * block_size])\n",
    "        t = einops.rearrange(t, '(bat block) -> bat block', bat=n_batches)\n",
    "        t = t.tolist()\n",
    "\n",
    "        result[k] = t\n",
    "\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"finetuned-tiny-shakespeare\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67594927",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a19e6",
   "metadata": {},
   "source": [
    "### HuggingFace Training (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb524c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', padding_side='left')\n",
    "# model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', device_map='auto', torch_dtype='auto')\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25135ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataset = load_dataset('yelp_review_full')\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-cased')\n",
    "\n",
    "def tokenize(examples):\n",
    "    return ft_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True).to('cuda')\n",
    "dataset = ft_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b281df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"yelp_review_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval = dataset[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf08ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66b65b",
   "metadata": {},
   "source": [
    "### HuggingFace Practice  \n",
    "*getting comfortable with HF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct', padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        'meta-llama/Llama-3.1-8B-Instruct', \n",
    "        device_map = 'auto',\n",
    "        torch_dtype = 'auto',\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'Jack n jill went up the hill to smoke a lot of pot',\n",
    "    'Beautiful world, where are you?',\n",
    "    'In the beginning, the universe was created. This was widely regarded as a bad move.',\n",
    "]\n",
    "\n",
    "out = tokenizer(sentences, padding=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "\n",
    "out2 = tokenizer.decode(out[0])\n",
    "\n",
    "out3 = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "print(out2 + '\\n')\n",
    "print(out)\n",
    "\n",
    "out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4faf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"What is 5+5? Reply with a single word.\"\n",
    "model_inputs = tokenizer(test_sentence, return_tensors='pt').to('cuda')\n",
    "\n",
    "generated = model.generate(**model_inputs, max_new_tokens=100, temperature=0.3, repetition_penalty= 2.0)\n",
    "out = tokenizer.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47eaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly, helpful chatbot.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is 5+5? Answer with a single word.\"},\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=50)\n",
    "print(tokenizer.batch_decode(generated_ids)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb4c3c",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97028351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda333b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LoRA configuration object\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # type of task to train on\n",
    "    inference_mode=False, # set to False for training\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e871434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(lora_config, adapter_name=\"lora_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9b376",
   "metadata": {},
   "source": [
    "### Async Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f066329",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def task1():\n",
    "    print(\"Task 1: Start\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Task 1: End\")\n",
    "\n",
    "async def task2():\n",
    "    print(\"Task 2: Start\")\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"Task 2: End\")\n",
    "\n",
    "# async def main():\n",
    "await asyncio.gather(task1(), task2())\n",
    "\n",
    "# await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_async():\n",
    "    prompt = 'hello world'\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    response = await client.responses.create(\n",
    "        model = 'gpt-4.1-mini',\n",
    "        input = prompt,\n",
    "    )\n",
    "\n",
    "    text_out = response.output[-1].content[0].text\n",
    "    return text_out\n",
    "\n",
    "def chat_reg():\n",
    "    prompt = 'hello world'\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model = 'gpt-4.1-mini',\n",
    "        input = prompt,\n",
    "    )\n",
    "\n",
    "    text_out = response.output[-1].content[0].text\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d13db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat_async()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
